<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="杨小鹤">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/08/09/ai/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="一、人工智能## 1.1 什么是人工智能总述：像人一样思考 像人一样行动 合理的思考 合理地行动### 1.1.1 像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过">
<meta property="og:type" content="article">
<meta property="og:title" content="杨小鹤">
<meta property="og:url" content="http://example.com/2023/08/09/ai/index.html">
<meta property="og:site_name" content="杨小鹤">
<meta property="og:description" content="一、人工智能## 1.1 什么是人工智能总述：像人一样思考 像人一样行动 合理的思考 合理地行动### 1.1.1 像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-08T18:07:04.104Z">
<meta property="article:modified_time" content="2023-08-08T18:06:50.781Z">
<meta property="article:author" content="Yang Xiaohe">
<meta name="twitter:card" content="summary">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/zhuzhuxia.jpg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/zhuzhuxia.jpg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/zhuzhuxia.jpg">
    <!--- Page Info-->
    
    <title>
        
        GGbond&#39; blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"en"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"GGbond","subtitle":{"text":[],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.3.0","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">
    

    

    <div class="main-content-container">
        

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/tushezhu.jpg">
                </a>
            
            <a class="logo-title" href="/">
                
                GGbond&#39; blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                <h1 class="article-title-regular"></h1>
            
            </div>
            
                    
        
        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/yongquanzhu.jpg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">杨小鹤</span>
                        
                            <span class="author-label">Lv1</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-08-09 02:07:04</span>
        <span class="mobile">2023-08-09 02:07:04</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-08-09 02:06:50</span>
            <span class="mobile">2023-08-09 02:06:50</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h1 id="一、人工智能-1-1-什么是人工智能总述：像人一样思考-像人一样行动-合理的思考-合理地行动-1-1-1-像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过图灵测而研究AI不是一定的，例如航空工程不会吧目标定义为制造为能完全像鸽子一样飞行的机器，以至他们可以骗过其他鸽子-1-1-2-像人一样思考：认知建模的途径前提确定人是如何思考的：-通过内省（捕获我们的思维过程）-通过心理实验（观察工作中的一个人）-通过脑成像（观察工作中的头脑）认知科学（cognitive-science-这个交叉学科领域把来自-AI的计算机模型与来自心理学的实验技术相结合试图构建一种精确且可测试的人类思维理论-1-1-3-合理的思考：思维法则的途径三段论（亚里士多德）：在给定正确前提时总产生正确结论的论证结构提供了模式。-开创了逻辑学因此产生人工智能中的逻辑主义：希望能用逻辑来解决问题，给世上各个对象之间陈述制定一种精确的表示法-1-1-4-合理地行动：合理agent的途径Agent：能够行动的某种东西Rational-Agent：合理Agent是一个为了实现最佳结果，或者当存在不确定性时，为了实现最佳期望结果而行动的agent。合理agent的途径与其他途径相比有两个优点：因为正确的推理只是实现合理性的几种可能的机制之一且更经得起检验。-1-2-人工智能的基础-1-2-1-哲学大致历史的亚里士多德：阐述支配头脑理性部分的一组精确规则的人。Ramon-Lull：有用的推理可以用机械人造物实现Thomas-Hobbes：提出推理就像数值计算达芬奇：设计了一台机械计算器……笛卡尔是二元论的支持者：他认为人类头脑存在一部分在自然之外不受物理定律支配的东西-动物无之后对二元论的替换物是唯物主义（materialism）经验主义始于培根-然后再大卫休谟提出了归纳原理-：一般规则通过揭示规则中元素之间的重复关联来获得逻辑实证主义：所有知识都可用最终与对应于感知输入的观察语句相联系的逻辑理论来刻画。头脑的哲学描述中的最后元素是知识与行动之间的联系。这个问题对人工智能是极其重要的，因为智能既要求推理又要求行动。亚里士多德提出：通过目标与行动结果的知识之间的逻辑关系来证明行动是正当的。后人利用这种算法成立了线性回归-1-2-2-数学三个领域：逻辑-计算和概率逻辑上：布尔逻辑-弗雷格扩展了布尔逻辑这便是现在使用的一阶逻辑。第一个不平凡算法：最大公约数-欧几里得哥德尔的不完备性定理：一样强的任何形式理论中都存在不可判断的真语句但是有很多问题是不可及算的易处理性的概念具有更大的影响：相比于可判定性和可计算性不易处理：如果解决一个问题的实例所需时间随实例的规模成指数级地增长，那么该问题称为不易处理的如何确定不易处理的问题：斯蒂文库克和卡普：NP-completeness-证明了存在大量经典组合搜索与推理问题是-NP-完全的。NP-完全问题类可归约到的任何问题类可能就是不易处理的（一般认为np完全是不易处理的）概率：按照赌博事件的可能结果来描述它贝叶斯的规则构成了人工智能系统中大多数用于不确定推理的现代方法的基础-1-2-3-经济学决策理论：-把概率理论和效用理论结合起来，为在不确定情况下，做出决策提供了一个形式化且完整的框架。-1-2-4-神经科学-1-2-5-心理学-1-2-6-计算机工程-二-智能agent-2-1-agent-和环境agent-接受键盘敲击、文件内容和网络数据包作为传感器输入Agent-的感知序列是该-Agent-所收到的所有输入数据的完整历史。Agent-函数描述了-Agent-的行为，它将任意给定感知序列映射为行动Agent-程序则是具体实现，它在一些物理系统内部运行。就是函数是表达，程序是执行-2-2-好的行为：理性的概念理性Agent是做事正确的Agent。渴望利用性能度量表述，它对环境状态的任何给定序列进行评估。作为一般原则，最好根据实际在环境中希望得到的结果来设计性能度量，而不是根据-Agent-表现出的行为。-2-2-1-理性理性判断的四方面：定义成功标准的性能度量-Agent对环境的先验知识-Agent可以完成的行动-Agent截止到此时的感知序列从来得到理性的定义：对每一个可能的感知序列，根据已知的感知序列提供的证据和-Agent-具有的先验知识，理性-Agent-应该选择能使其性能度量最大化的行动。-2-2-2-全知者-学习和自主性全知：一个全知的-Agent-明确地知道它的行动产生的实际结果并且做出相应的动作理性是使期望的性能最大化，而完美是使实际的性能最大化。对理性的定义并不要求全知，因为理性的选择只依赖于到当时为止的感知序列。信息收集：为了修改未来的感知信息而采取行动。是理性的重要部分Agent-依赖于设计人员的先验知识而不是它自身的感知信息，这种情况我们会说该Agent-缺乏自主性。理性的Agent应该是自主的：应该学习以弥补不完整的或者不正确的先验知识。-2-3-环境的性质如何构建理性Agent，首先考虑任务环境-2-3-1-任务环境的规范描述Agent-的理性，我们必须规定性能度量、环境以及-Agent的执行器和传感器。把所有这些归在一起，都属于任务环境。根据首字母缩写，我们称之为-PEAS-描述（Performance-性能），-Environment-环境），-Actuators-执行器），-Sensors-传感器））。-2-3-2对任务环境进行分类，这些维度很大程度上决定了Agent的设计。完全可观察的与部分可观察的：-可观察的：在每个时间点上都能获取环境的完整状态，那么环境就是可观察的。-如果传感器能够检测所有与行动决策下那个关心的信息，那么该任务环境是完全-有效可观察的。单Agent与多Agent：-关键的区别在于-B-的行为是否寻求让依赖于-Agent-A-的行为的性能度量值最大化确定的与随机的：-如果环境的下一个状态完全取决于当前状态和Agent执行的动作，那么我们说该环境是确定的，否则他是随机的。片段的与延续式的：-在片段式的任务环境中，Agent-的经历被分成了一个个原子片段。在每个片段中-Agent-感知信息并完成单个行动。关键的是，下一个片段不依赖于以前的片段中采取的行动。很多分类任务属于片段式的静态的与动态的：-如果环境在Agent-计算的时候会变化，那么是动态的。离散的与连续的：-环境的状态、时间的处理方式以及Agent的感知信息和行动，都有离散-连续之分。已知的和未知的：-这种区分指的不是环境本身，指的是Agent的只是状态，这里的只是则是值环境的“物理法则”。-2-4-Agent的结构AI的任务是设计Agent程序，它实现的是把感知信息映射到行动的Agent函数。假设该程序要在某个具备无力传感器和执行器的计算装置上运行，我们成为体系结构。Agent-体系结构-程序-2-4-1Agent程序-Agent-程序都具有同样的框架：输入为从传感器得到的当前感知信息，返回的是执行器的行动抉择-2-4-2-简单反射Agent简单反着Agent：-是最简单的种类-基于当前的感知选择行动，不关注感知历史。即符合条件那么就执行，这是条件-行为规则。如果什么什么-那么怎么怎么缺点：智能有限-会陷入无限的循环解决：Agent的行动能够随机化-2-4-3-基于模型的反射Agent处理部分可观测环境的最有效途径是让Agent跟踪记录现在看不到的那部分世界。即Agent应该根据感知历史维持内部状态，从而至少反映出当前状态看不到的信息。需要在程序中加入两种类型的知识：首先需要知道世界是如何独立于Agent而发展的信息-其次，知道Aagent自身的行动如何影响世界的信息。这种模型的Agent被称为基于模型的Agent关于-Agent-目的地的事实才是真正的-Agent-内部状态的一个方面-2-4-4-基于目标的AgentAgent还需要目标信息来描述想要达到的状况搜索和规划是寻找达成Agent目标的行动序列的人工智能与条件-行动不同，因为他考虑了未来尽管效率低，但是更加灵活-2-4-5-基于效用的Agent仅靠目标不足以生成高品质的行为。效用：Agent的效用函数是性能度量的内在化，如果内在的效用函数和外在的性能度量是和谐的，那么选择最大效用行动的Agent根据外在的性能度量也是理性的第一，当多个目标互相冲突时，只有其中一些目标可以达到时（例如，速度和安全性-，效用函数可以在它们之间适当的折中。第二，当-Agent-有几个目标，但没有一个有把握达到时，效用函数可以根据目标的重要性对成功的似然率加权理性的基于效用的-Agent-选择使其期望效用最大化的行动-2-4-6-学习Agent学习-Agent-可以被划分为-4-个概念上的组件：Critic-Learning-element-problem-generator-Performance-element，重点体现在学习元件和性能元件。学习元件负责改进提高，而性能元件负责选择外部行动。性能元件是我们前面考虑的整个-Agent-它接受感知信息并决策。学习元件利用来自评判元件的反馈评价-Agent-做得如何，并确定应该如何修改性能元件以便将来做得更好学习元件的设计很大程度上依赖于性能元件的设计评判元件根据固定的性能标准告诉学习元件-Agent-的运转情况最后一个组件是问题产生器。它负责可以得到新的和有信息的经验的行动提议-2-4-7-Agent程序的各组件如何工作我们将表示放置在不断增长的复杂度和表达能力的轴线上：原子、要素和结构-问题求解-第三章-通过搜索进行问题求解问题求解-Agent-使用原子（atomic-表示：世界的状态被视为一个整体，对问题求解算法而言没有可见的内部结构。使用更先进的要素化或结构化表示的基于目标的-Agent-通常被称为规划-Agent无信息的搜索算法：除了问题定义本身没有任何其他信息有信息的搜索算法：可以利用给定的值是引导能够有效的找到解-3-1-问题求解agent三步走：形式化、搜索、执行形式化：帮助-Agent-组织行动序列，以达到最终目标。基于当前的情形和-Agent-的性能度量进行目标形式化（goal-formulation-是问题求解的第一个步骤。问题形式化（problem-formulation-是在给定目标下确定需要考虑哪些行动和状态的过程搜索：为达到目标，寻找这样的行动序列的过程被称为搜索：搜索算法的输入是问题，输出是问题的解，以行动序列的形式返回问题的解。执行：执行建议的行动开环系统：十分确定行动的后果是什么，它无视它的感知信息。-3-1-1-良定义的问题以及解一个问题可以用5个组成部分形式化地描述：1-Agent的初始状态2-描述Agent的可能行动，给定一个特殊状态s，Action（s）返回在状态s下可以执行的动作集合。3-对每个行动的描述：即转移模型，这个状态下在这样的操作可以去到哪里。4-目标测试-确定给定的状态是不是目标状态。有时候目标状态是一个显式集合，测试只需简单检查给定的状态是否在目标状态集合中5-路径耗散：路径耗散函数为每条路径赋一个耗散值，即甲醛百年。问题求解Agent选择能反映它自己的性能度量的耗散函数。-3-1-2-问题的形式化一般情况下都要抽象，抽象成数学问题。要对状态描述抽象，要对行动进行抽象。如何精确的定义合适的抽象层次：选择抽象化的状态和行动。如果我们能够把任何抽象解扩展成为更细节的世界中的解，这种抽象就是有效的-3-2-问题实例分别讨论玩具问题和现实世界问题。-3-2-1-玩具问题讨论了吸尘器的具体应用，以及八数码问题以及八皇后问题（具体实例）对于八皇后这类的问题：这类问题的形式化主要分为两类：增量形式化（在过程中是从空状态开始的）、完整状态形式化不论这两种哪种，都无需考虑路径消耗，只需要考虑最终状态。增量形式话可以如下考虑：状态：初始状态：行动：转移模型：目标测试：状态：行动：还有玩具问题：由Donald-Knuth提出，只用数字4，一个由阶乘、平方根和取整构成的操作序列可以得到任意的正整数。-3-2-2-现实世界问题寻径问题旅行问题旅行商问题VLSI布线问题机器人导航问题-3-3-通过搜索求解进行形式化后，对问题进行求解：一个解是一个行动序列，所以搜索算法的工作就是考虑各种可能的行动数列。搜索树：从根节点开始，连线表示行动，节点对应空间中的状态。问题：如何选择将要扩展的状态L-即搜索策略-以避免冗余状态。"><a href="#一、人工智能-1-1-什么是人工智能总述：像人一样思考-像人一样行动-合理的思考-合理地行动-1-1-1-像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过图灵测而研究AI不是一定的，例如航空工程不会吧目标定义为制造为能完全像鸽子一样飞行的机器，以至他们可以骗过其他鸽子-1-1-2-像人一样思考：认知建模的途径前提确定人是如何思考的：-通过内省（捕获我们的思维过程）-通过心理实验（观察工作中的一个人）-通过脑成像（观察工作中的头脑）认知科学（cognitive-science-这个交叉学科领域把来自-AI的计算机模型与来自心理学的实验技术相结合试图构建一种精确且可测试的人类思维理论-1-1-3-合理的思考：思维法则的途径三段论（亚里士多德）：在给定正确前提时总产生正确结论的论证结构提供了模式。-开创了逻辑学因此产生人工智能中的逻辑主义：希望能用逻辑来解决问题，给世上各个对象之间陈述制定一种精确的表示法-1-1-4-合理地行动：合理agent的途径Agent：能够行动的某种东西Rational-Agent：合理Agent是一个为了实现最佳结果，或者当存在不确定性时，为了实现最佳期望结果而行动的agent。合理agent的途径与其他途径相比有两个优点：因为正确的推理只是实现合理性的几种可能的机制之一且更经得起检验。-1-2-人工智能的基础-1-2-1-哲学大致历史的亚里士多德：阐述支配头脑理性部分的一组精确规则的人。Ramon-Lull：有用的推理可以用机械人造物实现Thomas-Hobbes：提出推理就像数值计算达芬奇：设计了一台机械计算器……笛卡尔是二元论的支持者：他认为人类头脑存在一部分在自然之外不受物理定律支配的东西-动物无之后对二元论的替换物是唯物主义（materialism）经验主义始于培根-然后再大卫休谟提出了归纳原理-：一般规则通过揭示规则中元素之间的重复关联来获得逻辑实证主义：所有知识都可用最终与对应于感知输入的观察语句相联系的逻辑理论来刻画。头脑的哲学描述中的最后元素是知识与行动之间的联系。这个问题对人工智能是极其重要的，因为智能既要求推理又要求行动。亚里士多德提出：通过目标与行动结果的知识之间的逻辑关系来证明行动是正当的。后人利用这种算法成立了线性回归-1-2-2-数学三个领域：逻辑-计算和概率逻辑上：布尔逻辑-弗雷格扩展了布尔逻辑这便是现在使用的一阶逻辑。第一个不平凡算法：最大公约数-欧几里得哥德尔的不完备性定理：一样强的任何形式理论中都存在不可判断的真语句但是有很多问题是不可及算的易处理性的概念具有更大的影响：相比于可判定性和可计算性不易处理：如果解决一个问题的实例所需时间随实例的规模成指数级地增长，那么该问题称为不易处理的如何确定不易处理的问题：斯蒂文库克和卡普：NP-completeness-证明了存在大量经典组合搜索与推理问题是-NP-完全的。NP-完全问题类可归约到的任何问题类可能就是不易处理的（一般认为np完全是不易处理的）概率：按照赌博事件的可能结果来描述它贝叶斯的规则构成了人工智能系统中大多数用于不确定推理的现代方法的基础-1-2-3-经济学决策理论：-把概率理论和效用理论结合起来，为在不确定情况下，做出决策提供了一个形式化且完整的框架。-1-2-4-神经科学-1-2-5-心理学-1-2-6-计算机工程-二-智能agent-2-1-agent-和环境agent-接受键盘敲击、文件内容和网络数据包作为传感器输入Agent-的感知序列是该-Agent-所收到的所有输入数据的完整历史。Agent-函数描述了-Agent-的行为，它将任意给定感知序列映射为行动Agent-程序则是具体实现，它在一些物理系统内部运行。就是函数是表达，程序是执行-2-2-好的行为：理性的概念理性Agent是做事正确的Agent。渴望利用性能度量表述，它对环境状态的任何给定序列进行评估。作为一般原则，最好根据实际在环境中希望得到的结果来设计性能度量，而不是根据-Agent-表现出的行为。-2-2-1-理性理性判断的四方面：定义成功标准的性能度量-Agent对环境的先验知识-Agent可以完成的行动-Agent截止到此时的感知序列从来得到理性的定义：对每一个可能的感知序列，根据已知的感知序列提供的证据和-Agent-具有的先验知识，理性-Agent-应该选择能使其性能度量最大化的行动。-2-2-2-全知者-学习和自主性全知：一个全知的-Agent-明确地知道它的行动产生的实际结果并且做出相应的动作理性是使期望的性能最大化，而完美是使实际的性能最大化。对理性的定义并不要求全知，因为理性的选择只依赖于到当时为止的感知序列。信息收集：为了修改未来的感知信息而采取行动。是理性的重要部分Agent-依赖于设计人员的先验知识而不是它自身的感知信息，这种情况我们会说该Agent-缺乏自主性。理性的Agent应该是自主的：应该学习以弥补不完整的或者不正确的先验知识。-2-3-环境的性质如何构建理性Agent，首先考虑任务环境-2-3-1-任务环境的规范描述Agent-的理性，我们必须规定性能度量、环境以及-Agent的执行器和传感器。把所有这些归在一起，都属于任务环境。根据首字母缩写，我们称之为-PEAS-描述（Performance-性能），-Environment-环境），-Actuators-执行器），-Sensors-传感器））。-2-3-2对任务环境进行分类，这些维度很大程度上决定了Agent的设计。完全可观察的与部分可观察的：-可观察的：在每个时间点上都能获取环境的完整状态，那么环境就是可观察的。-如果传感器能够检测所有与行动决策下那个关心的信息，那么该任务环境是完全-有效可观察的。单Agent与多Agent：-关键的区别在于-B-的行为是否寻求让依赖于-Agent-A-的行为的性能度量值最大化确定的与随机的：-如果环境的下一个状态完全取决于当前状态和Agent执行的动作，那么我们说该环境是确定的，否则他是随机的。片段的与延续式的：-在片段式的任务环境中，Agent-的经历被分成了一个个原子片段。在每个片段中-Agent-感知信息并完成单个行动。关键的是，下一个片段不依赖于以前的片段中采取的行动。很多分类任务属于片段式的静态的与动态的：-如果环境在Agent-计算的时候会变化，那么是动态的。离散的与连续的：-环境的状态、时间的处理方式以及Agent的感知信息和行动，都有离散-连续之分。已知的和未知的：-这种区分指的不是环境本身，指的是Agent的只是状态，这里的只是则是值环境的“物理法则”。-2-4-Agent的结构AI的任务是设计Agent程序，它实现的是把感知信息映射到行动的Agent函数。假设该程序要在某个具备无力传感器和执行器的计算装置上运行，我们成为体系结构。Agent-体系结构-程序-2-4-1Agent程序-Agent-程序都具有同样的框架：输入为从传感器得到的当前感知信息，返回的是执行器的行动抉择-2-4-2-简单反射Agent简单反着Agent：-是最简单的种类-基于当前的感知选择行动，不关注感知历史。即符合条件那么就执行，这是条件-行为规则。如果什么什么-那么怎么怎么缺点：智能有限-会陷入无限的循环解决：Agent的行动能够随机化-2-4-3-基于模型的反射Agent处理部分可观测环境的最有效途径是让Agent跟踪记录现在看不到的那部分世界。即Agent应该根据感知历史维持内部状态，从而至少反映出当前状态看不到的信息。需要在程序中加入两种类型的知识：首先需要知道世界是如何独立于Agent而发展的信息-其次，知道Aagent自身的行动如何影响世界的信息。这种模型的Agent被称为基于模型的Agent关于-Agent-目的地的事实才是真正的-Agent-内部状态的一个方面-2-4-4-基于目标的AgentAgent还需要目标信息来描述想要达到的状况搜索和规划是寻找达成Agent目标的行动序列的人工智能与条件-行动不同，因为他考虑了未来尽管效率低，但是更加灵活-2-4-5-基于效用的Agent仅靠目标不足以生成高品质的行为。效用：Agent的效用函数是性能度量的内在化，如果内在的效用函数和外在的性能度量是和谐的，那么选择最大效用行动的Agent根据外在的性能度量也是理性的第一，当多个目标互相冲突时，只有其中一些目标可以达到时（例如，速度和安全性-，效用函数可以在它们之间适当的折中。第二，当-Agent-有几个目标，但没有一个有把握达到时，效用函数可以根据目标的重要性对成功的似然率加权理性的基于效用的-Agent-选择使其期望效用最大化的行动-2-4-6-学习Agent学习-Agent-可以被划分为-4-个概念上的组件：Critic-Learning-element-problem-generator-Performance-element，重点体现在学习元件和性能元件。学习元件负责改进提高，而性能元件负责选择外部行动。性能元件是我们前面考虑的整个-Agent-它接受感知信息并决策。学习元件利用来自评判元件的反馈评价-Agent-做得如何，并确定应该如何修改性能元件以便将来做得更好学习元件的设计很大程度上依赖于性能元件的设计评判元件根据固定的性能标准告诉学习元件-Agent-的运转情况最后一个组件是问题产生器。它负责可以得到新的和有信息的经验的行动提议-2-4-7-Agent程序的各组件如何工作我们将表示放置在不断增长的复杂度和表达能力的轴线上：原子、要素和结构-问题求解-第三章-通过搜索进行问题求解问题求解-Agent-使用原子（atomic-表示：世界的状态被视为一个整体，对问题求解算法而言没有可见的内部结构。使用更先进的要素化或结构化表示的基于目标的-Agent-通常被称为规划-Agent无信息的搜索算法：除了问题定义本身没有任何其他信息有信息的搜索算法：可以利用给定的值是引导能够有效的找到解-3-1-问题求解agent三步走：形式化、搜索、执行形式化：帮助-Agent-组织行动序列，以达到最终目标。基于当前的情形和-Agent-的性能度量进行目标形式化（goal-formulation-是问题求解的第一个步骤。问题形式化（problem-formulation-是在给定目标下确定需要考虑哪些行动和状态的过程搜索：为达到目标，寻找这样的行动序列的过程被称为搜索：搜索算法的输入是问题，输出是问题的解，以行动序列的形式返回问题的解。执行：执行建议的行动开环系统：十分确定行动的后果是什么，它无视它的感知信息。-3-1-1-良定义的问题以及解一个问题可以用5个组成部分形式化地描述：1-Agent的初始状态2-描述Agent的可能行动，给定一个特殊状态s，Action（s）返回在状态s下可以执行的动作集合。3-对每个行动的描述：即转移模型，这个状态下在这样的操作可以去到哪里。4-目标测试-确定给定的状态是不是目标状态。有时候目标状态是一个显式集合，测试只需简单检查给定的状态是否在目标状态集合中5-路径耗散：路径耗散函数为每条路径赋一个耗散值，即甲醛百年。问题求解Agent选择能反映它自己的性能度量的耗散函数。-3-1-2-问题的形式化一般情况下都要抽象，抽象成数学问题。要对状态描述抽象，要对行动进行抽象。如何精确的定义合适的抽象层次：选择抽象化的状态和行动。如果我们能够把任何抽象解扩展成为更细节的世界中的解，这种抽象就是有效的-3-2-问题实例分别讨论玩具问题和现实世界问题。-3-2-1-玩具问题讨论了吸尘器的具体应用，以及八数码问题以及八皇后问题（具体实例）对于八皇后这类的问题：这类问题的形式化主要分为两类：增量形式化（在过程中是从空状态开始的）、完整状态形式化不论这两种哪种，都无需考虑路径消耗，只需要考虑最终状态。增量形式话可以如下考虑：状态：初始状态：行动：转移模型：目标测试：状态：行动：还有玩具问题：由Donald-Knuth提出，只用数字4，一个由阶乘、平方根和取整构成的操作序列可以得到任意的正整数。-3-2-2-现实世界问题寻径问题旅行问题旅行商问题VLSI布线问题机器人导航问题-3-3-通过搜索求解进行形式化后，对问题进行求解：一个解是一个行动序列，所以搜索算法的工作就是考虑各种可能的行动数列。搜索树：从根节点开始，连线表示行动，节点对应空间中的状态。问题：如何选择将要扩展的状态L-即搜索策略-以避免冗余状态。" class="headerlink" title="一、人工智能## 1.1 什么是人工智能总述：像人一样思考 像人一样行动 合理的思考 合理地行动### 1.1.1 像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过图灵测而研究AI不是一定的，例如航空工程不会吧目标定义为制造为能完全像鸽子一样飞行的机器，以至他们可以骗过其他鸽子 ### 1.1.2 像人一样思考：认知建模的途径前提确定人是如何思考的： 通过内省（捕获我们的思维过程）                                          通过心理实验（观察工作中的一个人）                                         通过脑成像（观察工作中的头脑）认知科学（cognitive science) 这个交叉学科领域把来自 AI的计算机模型与来自心理学的实验技术相结合试图构建一种精确且可测试的人类思维理论### 1.1.3 合理的思考：思维法则的途径三段论（亚里士多德）：在给定正确前提时总产生正确结论的论证结构提供了模式。 开创了逻辑学因此产生人工智能中的逻辑主义：希望能用逻辑来解决问题，给世上各个对象之间陈述制定一种精确的表示法### 1.1.4 合理地行动：合理agent的途径Agent：能够行动的某种东西Rational Agent：合理Agent是一个为了实现最佳结果，或者当存在不确定性时，为了实现最佳期望结果而行动的agent。合理agent的途径与其他途径相比有两个优点：因为正确的推理只是实现合理性的几种可能的机制之一且更经得起检验。## 1.2 人工智能的基础### 1.2.1 哲学大致历史的亚里士多德：阐述支配头脑理性部分的一组精确规则的人。Ramon Lull：有用的推理可以用机械人造物实现Thomas Hobbes：提出推理就像数值计算达芬奇：设计了一台机械计算器……笛卡尔是二元论的支持者：他认为人类头脑存在一部分在自然之外不受物理定律支配的东西 动物无之后对二元论的替换物是唯物主义（materialism）经验主义始于培根 然后再大卫休谟提出了归纳原理 ：一般规则通过揭示规则中元素之间的重复关联来获得逻辑实证主义：所有知识都可用最终与对应于感知输入的观察语句相联系的逻辑理论来刻画。头脑的哲学描述中的最后元素是知识与行动之间的联系。这个问题对人工智能是极其重要的，因为智能既要求推理又要求行动。亚里士多德提出：通过目标与行动结果的知识之间的逻辑关系来证明行动是正当的。后人利用这种算法成立了线性回归### 1.2.2 数学三个领域：逻辑 计算和概率逻辑上：布尔逻辑 弗雷格扩展了布尔逻辑这便是现在使用的一阶逻辑。第一个不平凡算法：最大公约数 欧几里得哥德尔的不完备性定理：一样强的任何形式理论中都存在不可判断的真语句但是有很多问题是不可及算的易处理性的概念具有更大的影响：相比于可判定性和可计算性不易处理：如果解决一个问题的实例所需时间随实例的规模成指数级地增长，那么该问题称为不易处理的如何确定不易处理的问题：斯蒂文库克和卡普：NP-completeness 证明了存在大量经典组合搜索与推理问题是 NP-完全的。NP-完全问题类可归约到的任何问题类可能就是不易处理的（一般认为np完全是不易处理的）概率：按照赌博事件的可能结果来描述它贝叶斯的规则构成了人工智能系统中大多数用于不确定推理的现代方法的基础### 1.2.3 经济学决策理论： 把概率理论和效用理论结合起来，为在不确定情况下，做出决策提供了一个形式化且完整的框架。### 1.2.4 神经科学### 1.2.5 心理学### 1.2.6 计算机工程# 二 智能agent## 2.1 agent 和环境agent 接受键盘敲击、文件内容和网络数据包作为传感器输入Agent 的感知序列是该 Agent 所收到的所有输入数据的完整历史。Agent 函数描述了 Agent 的行为，它将任意给定感知序列映射为行动Agent 程序则是具体实现，它在一些物理系统内部运行。就是函数是表达，程序是执行## 2.2 好的行为：理性的概念理性Agent是做事正确的Agent。渴望利用性能度量表述，它对环境状态的任何给定序列进行评估。作为一般原则，最好根据实际在环境中希望得到的结果来设计性能度量，而不是根据 Agent 表现出的行为。### 2.2.1 理性理性判断的四方面：定义成功标准的性能度量  Agent对环境的先验知识  Agent可以完成的行动  Agent截止到此时的感知序列从来得到理性的定义：对每一个可能的感知序列，根据已知的感知序列提供的证据和 Agent 具有的先验知识，理性 Agent 应该选择能使其性能度量最大化的行动。### 2.2.2 全知者 学习和自主性全知：一个全知的 Agent 明确地知道它的行动产生的实际结果并且做出相应的动作理性是使期望的性能最大化，而完美是使实际的性能最大化。对理性的定义并不要求全知，因为理性的选择只依赖于到当时为止的感知序列。信息收集：为了修改未来的感知信息而采取行动。是理性的重要部分Agent 依赖于设计人员的先验知识而不是它自身的感知信息，这种情况我们会说该Agent 缺乏自主性。理性的Agent应该是自主的：应该学习以弥补不完整的或者不正确的先验知识。## 2.3 环境的性质如何构建理性Agent，首先考虑任务环境### 2.3.1 任务环境的规范描述Agent 的理性，我们必须规定性能度量、环境以及 Agent的执行器和传感器。把所有这些归在一起，都属于任务环境。根据首字母缩写，我们称之为 PEAS 描述（Performance (性能）， Environment (环境）， Actuators (执行器）， Sensors(传感器））。### 2.3.2对任务环境进行分类，这些维度很大程度上决定了Agent的设计。完全可观察的与部分可观察的：    可观察的：在每个时间点上都能获取环境的完整状态，那么环境就是可观察的。                如果传感器能够检测所有与行动决策下那个关心的信息，那么该任务环境是完全                   有效可观察的。单Agent与多Agent：    关键的区别在于 B 的行为是否寻求让依赖于 Agent A 的行为的性能度量值最大化确定的与随机的：    如果环境的下一个状态完全取决于当前状态和Agent执行的动作，那么我们说该环境是确定的，否则他是随机的。片段的与延续式的：    在片段式的任务环境中，Agent 的经历被分成了一个个原子片段。在每个片段中 Agent 感知信息并完成单个行动。关键的是，下一个片段不依赖于以前的片段中采取的行动。很多分类任务属于片段式的静态的与动态的：    如果环境在Agent 计算的时候会变化，那么是动态的。离散的与连续的：    环境的状态、时间的处理方式以及Agent的感知信息和行动，都有离散&#x2F;连续之分。已知的和未知的：    这种区分指的不是环境本身，指的是Agent的只是状态，这里的只是则是值环境的“物理法则”。    ## 2.4 Agent的结构AI的任务是设计Agent程序，它实现的是把感知信息映射到行动的Agent函数。假设该程序要在某个具备无力传感器和执行器的计算装置上运行，我们成为体系结构。Agent &#x3D; 体系结构+程序### 2.4.1Agent程序 Agent 程序都具有同样的框架：输入为从传感器得到的当前感知信息，返回的是执行器的行动抉择### 2.4.2 简单反射Agent简单反着Agent： 是最简单的种类 基于当前的感知选择行动，不关注感知历史。即符合条件那么就执行，这是条件-行为规则。如果什么什么 那么怎么怎么缺点：智能有限 会陷入无限的循环解决：Agent的行动能够随机化 ### 2.4.3 基于模型的反射Agent处理部分可观测环境的最有效途径是让Agent跟踪记录现在看不到的那部分世界。即Agent应该根据感知历史维持内部状态，从而至少反映出当前状态看不到的信息。需要在程序中加入两种类型的知识：首先需要知道世界是如何独立于Agent而发展的信息                                                    其次，知道Aagent自身的行动如何影响世界的信息。这种模型的Agent被称为基于模型的Agent关于 Agent 目的地的事实才是真正的 Agent 内部状态的一个方面### 2.4.4 基于目标的AgentAgent还需要目标信息来描述想要达到的状况搜索和规划是寻找达成Agent目标的行动序列的人工智能与条件-行动不同，因为他考虑了未来尽管效率低，但是更加灵活### 2.4.5 基于效用的Agent仅靠目标不足以生成高品质的行为。效用：Agent的效用函数是性能度量的内在化，如果内在的效用函数和外在的性能度量是和谐的，那么选择最大效用行动的Agent根据外在的性能度量也是理性的第一，当多个目标互相冲突时，只有其中一些目标可以达到时（例如，速度和安全性)，效用函数可以在它们之间适当的折中。第二，当 Agent 有几个目标，但没有一个有把握达到时，效用函数可以根据目标的重要性对成功的似然率加权理性的基于效用的 Agent 选择使其期望效用最大化的行动### 2.4.6 学习Agent学习 Agent 可以被划分为 4 个概念上的组件：Critic    Learning element  problem generator Performance element，重点体现在学习元件和性能元件。学习元件负责改进提高，而性能元件负责选择外部行动。性能元件是我们前面考虑的整个 Agent: 它接受感知信息并决策。学习元件利用来自评判元件的反馈评价 Agent 做得如何，并确定应该如何修改性能元件以便将来做得更好学习元件的设计很大程度上依赖于性能元件的设计评判元件根据固定的性能标准告诉学习元件 Agent 的运转情况最后一个组件是问题产生器。它负责可以得到新的和有信息的经验的行动提议### 2.4.7 Agent程序的各组件如何工作我们将表示放置在不断增长的复杂度和表达能力的轴线上：原子、要素和结构# 问题求解#第三章 通过搜索进行问题求解问题求解 Agent 使用原子（atomic) 表示：世界的状态被视为一个整体，对问题求解算法而言没有可见的内部结构。使用更先进的要素化或结构化表示的基于目标的 Agent, 通常被称为规划 Agent无信息的搜索算法：除了问题定义本身没有任何其他信息有信息的搜索算法：可以利用给定的值是引导能够有效的找到解## 3.1 问题求解agent三步走：形式化、搜索、执行形式化：帮助 Agent 组织行动序列，以达到最终目标。基于当前的情形和 Agent 的性能度量进行目标形式化（goal formulation) 是问题求解的第一个步骤。问题形式化（problem formulation)是在给定目标下确定需要考虑哪些行动和状态的过程搜索：为达到目标，寻找这样的行动序列的过程被称为搜索：搜索算法的输入是问题，输出是问题的解，以行动序列的形式返回问题的解。执行：执行建议的行动开环系统：十分确定行动的后果是什么，它无视它的感知信息。###3.1.1 良定义的问题以及解一个问题可以用5个组成部分形式化地描述：1.Agent的初始状态2.描述Agent的可能行动，给定一个特殊状态s，Action（s）返回在状态s下可以执行的动作集合。3.对每个行动的描述：即转移模型，这个状态下在这样的操作可以去到哪里。4.目标测试 确定给定的状态是不是目标状态。有时候目标状态是一个显式集合，测试只需简单检查给定的状态是否在目标状态集合中5.路径耗散：路径耗散函数为每条路径赋一个耗散值，即甲醛百年。问题求解Agent选择能反映它自己的性能度量的耗散函数。###3.1.2 问题的形式化一般情况下都要抽象，抽象成数学问题。要对状态描述抽象，要对行动进行抽象。如何精确的定义合适的抽象层次：选择抽象化的状态和行动。如果我们能够把任何抽象解扩展成为更细节的世界中的解，这种抽象就是有效的## 3.2 问题实例分别讨论玩具问题和现实世界问题。### 3.2.1 玩具问题讨论了吸尘器的具体应用，以及八数码问题以及八皇后问题（具体实例）对于八皇后这类的问题：这类问题的形式化主要分为两类：增量形式化（在过程中是从空状态开始的）、完整状态形式化不论这两种哪种，都无需考虑路径消耗，只需要考虑最终状态。增量形式话可以如下考虑：状态：初始状态：行动：转移模型：目标测试：状态：行动：还有玩具问题：由Donald Knuth提出，只用数字4，一个由阶乘、平方根和取整构成的操作序列可以得到任意的正整数。### 3.2.2 现实世界问题寻径问题旅行问题旅行商问题VLSI布线问题机器人导航问题## 3.3 通过搜索求解进行形式化后，对问题进行求解：一个解是一个行动序列，所以搜索算法的工作就是考虑各种可能的行动数列。搜索树：从根节点开始，连线表示行动，节点对应空间中的状态。问题：如何选择将要扩展的状态L:即搜索策略 以避免冗余状态。"></a>一、人工智能## 1.1 什么是人工智能总述：像人一样思考 像人一样行动 合理的思考 合理地行动### 1.1.1 像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过图灵测而研究AI不是一定的，例如航空工程不会吧目标定义为制造为能完全像鸽子一样飞行的机器，以至他们可以骗过其他鸽子 ### 1.1.2 像人一样思考：认知建模的途径前提确定人是如何思考的： 通过内省（捕获我们的思维过程）                                          通过心理实验（观察工作中的一个人）                                         通过脑成像（观察工作中的头脑）认知科学（cognitive science) 这个交叉学科领域把来自 AI的计算机模型与来自心理学的实验技术相结合试图构建一种精确且可测试的人类思维理论### 1.1.3 合理的思考：思维法则的途径三段论（亚里士多德）：在给定正确前提时总产生正确结论的论证结构提供了模式。 开创了逻辑学因此产生人工智能中的逻辑主义：希望能用逻辑来解决问题，给世上各个对象之间陈述制定一种精确的表示法### 1.1.4 合理地行动：合理agent的途径Agent：能够行动的某种东西Rational Agent：合理Agent是一个为了实现最佳结果，或者当存在不确定性时，为了实现最佳期望结果而行动的agent。合理agent的途径与其他途径相比有两个优点：因为正确的推理只是实现合理性的几种可能的机制之一且更经得起检验。## 1.2 人工智能的基础### 1.2.1 哲学大致历史的亚里士多德：阐述支配头脑理性部分的一组精确规则的人。Ramon Lull：有用的推理可以用机械人造物实现Thomas Hobbes：提出推理就像数值计算达芬奇：设计了一台机械计算器……笛卡尔是二元论的支持者：他认为人类头脑存在一部分在自然之外不受物理定律支配的东西 动物无之后对二元论的替换物是唯物主义（materialism）经验主义始于培根 然后再大卫休谟提出了归纳原理 ：一般规则通过揭示规则中元素之间的重复关联来获得逻辑实证主义：所有知识都可用最终与对应于感知输入的观察语句相联系的逻辑理论来刻画。头脑的哲学描述中的最后元素是知识与行动之间的联系。这个问题对人工智能是极其重要的，因为智能既要求推理又要求行动。亚里士多德提出：通过目标与行动结果的知识之间的逻辑关系来证明行动是正当的。后人利用这种算法成立了线性回归### 1.2.2 数学三个领域：逻辑 计算和概率逻辑上：布尔逻辑 弗雷格扩展了布尔逻辑这便是现在使用的一阶逻辑。第一个不平凡算法：最大公约数 欧几里得哥德尔的不完备性定理：一样强的任何形式理论中都存在不可判断的真语句但是有很多问题是不可及算的易处理性的概念具有更大的影响：相比于可判定性和可计算性不易处理：如果解决一个问题的实例所需时间随实例的规模成指数级地增长，那么该问题称为不易处理的如何确定不易处理的问题：斯蒂文库克和卡普：NP-completeness 证明了存在大量经典组合搜索与推理问题是 NP-完全的。NP-完全问题类可归约到的任何问题类可能就是不易处理的（一般认为np完全是不易处理的）概率：按照赌博事件的可能结果来描述它贝叶斯的规则构成了人工智能系统中大多数用于不确定推理的现代方法的基础### 1.2.3 经济学决策理论： 把概率理论和效用理论结合起来，为在不确定情况下，做出决策提供了一个形式化且完整的框架。### 1.2.4 神经科学### 1.2.5 心理学### 1.2.6 计算机工程# 二 智能agent## 2.1 agent 和环境agent 接受键盘敲击、文件内容和网络数据包作为传感器输入Agent 的感知序列是该 Agent 所收到的所有输入数据的完整历史。Agent 函数描述了 Agent 的行为，它将任意给定感知序列映射为行动Agent 程序则是具体实现，它在一些物理系统内部运行。就是函数是表达，程序是执行## 2.2 好的行为：理性的概念理性Agent是做事正确的Agent。渴望利用性能度量表述，它对环境状态的任何给定序列进行评估。作为一般原则，最好根据实际在环境中希望得到的结果来设计性能度量，而不是根据 Agent 表现出的行为。### 2.2.1 理性理性判断的四方面：定义成功标准的性能度量  Agent对环境的先验知识  Agent可以完成的行动  Agent截止到此时的感知序列从来得到理性的定义：对每一个可能的感知序列，根据已知的感知序列提供的证据和 Agent 具有的先验知识，理性 Agent 应该选择能使其性能度量最大化的行动。### 2.2.2 全知者 学习和自主性全知：一个全知的 Agent 明确地知道它的行动产生的实际结果并且做出相应的动作理性是使期望的性能最大化，而完美是使实际的性能最大化。对理性的定义并不要求全知，因为理性的选择只依赖于到当时为止的感知序列。信息收集：为了修改未来的感知信息而采取行动。是理性的重要部分Agent 依赖于设计人员的先验知识而不是它自身的感知信息，这种情况我们会说该Agent 缺乏自主性。理性的Agent应该是自主的：应该学习以弥补不完整的或者不正确的先验知识。## 2.3 环境的性质如何构建理性Agent，首先考虑任务环境### 2.3.1 任务环境的规范描述Agent 的理性，我们必须规定性能度量、环境以及 Agent的执行器和传感器。把所有这些归在一起，都属于任务环境。根据首字母缩写，我们称之为 PEAS 描述（Performance (性能）， Environment (环境）， Actuators (执行器）， Sensors(传感器））。### 2.3.2对任务环境进行分类，这些维度很大程度上决定了Agent的设计。完全可观察的与部分可观察的：    可观察的：在每个时间点上都能获取环境的完整状态，那么环境就是可观察的。                如果传感器能够检测所有与行动决策下那个关心的信息，那么该任务环境是完全                   有效可观察的。单Agent与多Agent：    关键的区别在于 B 的行为是否寻求让依赖于 Agent A 的行为的性能度量值最大化确定的与随机的：    如果环境的下一个状态完全取决于当前状态和Agent执行的动作，那么我们说该环境是确定的，否则他是随机的。片段的与延续式的：    在片段式的任务环境中，Agent 的经历被分成了一个个原子片段。在每个片段中 Agent 感知信息并完成单个行动。关键的是，下一个片段不依赖于以前的片段中采取的行动。很多分类任务属于片段式的静态的与动态的：    如果环境在Agent 计算的时候会变化，那么是动态的。离散的与连续的：    环境的状态、时间的处理方式以及Agent的感知信息和行动，都有离散&#x2F;连续之分。已知的和未知的：    这种区分指的不是环境本身，指的是Agent的只是状态，这里的只是则是值环境的“物理法则”。    ## 2.4 Agent的结构AI的任务是设计Agent程序，它实现的是把感知信息映射到行动的Agent函数。假设该程序要在某个具备无力传感器和执行器的计算装置上运行，我们成为体系结构。Agent &#x3D; 体系结构+程序### 2.4.1Agent程序 Agent 程序都具有同样的框架：输入为从传感器得到的当前感知信息，返回的是执行器的行动抉择### 2.4.2 简单反射Agent简单反着Agent： 是最简单的种类 基于当前的感知选择行动，不关注感知历史。即符合条件那么就执行，这是条件-行为规则。如果什么什么 那么怎么怎么缺点：智能有限 会陷入无限的循环解决：Agent的行动能够随机化 ### 2.4.3 基于模型的反射Agent处理部分可观测环境的最有效途径是让Agent跟踪记录现在看不到的那部分世界。即Agent应该根据感知历史维持内部状态，从而至少反映出当前状态看不到的信息。需要在程序中加入两种类型的知识：首先需要知道世界是如何独立于Agent而发展的信息                                                    其次，知道Aagent自身的行动如何影响世界的信息。这种模型的Agent被称为基于模型的Agent关于 Agent 目的地的事实才是真正的 Agent 内部状态的一个方面### 2.4.4 基于目标的AgentAgent还需要目标信息来描述想要达到的状况搜索和规划是寻找达成Agent目标的行动序列的人工智能与条件-行动不同，因为他考虑了未来尽管效率低，但是更加灵活### 2.4.5 基于效用的Agent仅靠目标不足以生成高品质的行为。效用：Agent的效用函数是性能度量的内在化，如果内在的效用函数和外在的性能度量是和谐的，那么选择最大效用行动的Agent根据外在的性能度量也是理性的第一，当多个目标互相冲突时，只有其中一些目标可以达到时（例如，速度和安全性)，效用函数可以在它们之间适当的折中。第二，当 Agent 有几个目标，但没有一个有把握达到时，效用函数可以根据目标的重要性对成功的似然率加权理性的基于效用的 Agent 选择使其期望效用最大化的行动### 2.4.6 学习Agent学习 Agent 可以被划分为 4 个概念上的组件：Critic    Learning element  problem generator Performance element，重点体现在学习元件和性能元件。学习元件负责改进提高，而性能元件负责选择外部行动。性能元件是我们前面考虑的整个 Agent: 它接受感知信息并决策。学习元件利用来自评判元件的反馈评价 Agent 做得如何，并确定应该如何修改性能元件以便将来做得更好学习元件的设计很大程度上依赖于性能元件的设计评判元件根据固定的性能标准告诉学习元件 Agent 的运转情况最后一个组件是问题产生器。它负责可以得到新的和有信息的经验的行动提议### 2.4.7 Agent程序的各组件如何工作我们将表示放置在不断增长的复杂度和表达能力的轴线上：原子、要素和结构# 问题求解#第三章 通过搜索进行问题求解问题求解 Agent 使用原子（atomic) 表示：世界的状态被视为一个整体，对问题求解算法而言没有可见的内部结构。使用更先进的要素化或结构化表示的基于目标的 Agent, 通常被称为规划 Agent无信息的搜索算法：除了问题定义本身没有任何其他信息有信息的搜索算法：可以利用给定的值是引导能够有效的找到解## 3.1 问题求解agent三步走：形式化、搜索、执行形式化：帮助 Agent 组织行动序列，以达到最终目标。基于当前的情形和 Agent 的性能度量进行目标形式化（goal formulation) 是问题求解的第一个步骤。问题形式化（problem formulation)是在给定目标下确定需要考虑哪些行动和状态的过程搜索：为达到目标，寻找这样的行动序列的过程被称为搜索：搜索算法的输入是问题，输出是问题的解，以行动序列的形式返回问题的解。执行：执行建议的行动开环系统：十分确定行动的后果是什么，它无视它的感知信息。###3.1.1 良定义的问题以及解一个问题可以用5个组成部分形式化地描述：1.Agent的初始状态2.描述Agent的可能行动，给定一个特殊状态s，Action（s）返回在状态s下可以执行的动作集合。3.对每个行动的描述：即转移模型，这个状态下在这样的操作可以去到哪里。4.目标测试 确定给定的状态是不是目标状态。有时候目标状态是一个显式集合，测试只需简单检查给定的状态是否在目标状态集合中5.路径耗散：路径耗散函数为每条路径赋一个耗散值，即甲醛百年。问题求解Agent选择能反映它自己的性能度量的耗散函数。###3.1.2 问题的形式化一般情况下都要抽象，抽象成数学问题。要对状态描述抽象，要对行动进行抽象。如何精确的定义合适的抽象层次：选择抽象化的状态和行动。如果我们能够把任何抽象解扩展成为更细节的世界中的解，这种抽象就是有效的## 3.2 问题实例分别讨论玩具问题和现实世界问题。### 3.2.1 玩具问题讨论了吸尘器的具体应用，以及八数码问题以及八皇后问题（具体实例）对于八皇后这类的问题：这类问题的形式化主要分为两类：增量形式化（在过程中是从空状态开始的）、完整状态形式化不论这两种哪种，都无需考虑路径消耗，只需要考虑最终状态。增量形式话可以如下考虑：状态：初始状态：行动：转移模型：目标测试：状态：行动：还有玩具问题：由Donald Knuth提出，只用数字4，一个由阶乘、平方根和取整构成的操作序列可以得到任意的正整数。### 3.2.2 现实世界问题寻径问题旅行问题旅行商问题VLSI布线问题机器人导航问题## 3.3 通过搜索求解进行形式化后，对问题进行求解：一个解是一个行动序列，所以搜索算法的工作就是考虑各种可能的行动数列。搜索树：从根节点开始，连线表示行动，节点对应空间中的状态。问题：如何选择将要扩展的状态L:即搜索策略 以避免冗余状态。</h1><pre><code>1. 避免探索冗余路径的方法是牢记曾经走过的路，所以给tree-search算法增加一个参数，这个数据结构称为探索集。（closed表）。用它记录每个已扩展过的节点，新生成的节点若与已经生成的某个结点相匹配的话，那么他会被丢弃而不是加入边缘集。新算法叫Graph-Search
</code></pre>
<h3 id="3-3-1-搜索算法的基础搜索算法需要一个数据结构来记录搜索树的构造过程。对树的每个结点-我们定义的数据结构包含四个元素："><a href="#3-3-1-搜索算法的基础搜索算法需要一个数据结构来记录搜索树的构造过程。对树的每个结点-我们定义的数据结构包含四个元素：" class="headerlink" title="3.3.1 搜索算法的基础搜索算法需要一个数据结构来记录搜索树的构造过程。对树的每个结点:我们定义的数据结构包含四个元素："></a>3.3.1 搜索算法的基础搜索算法需要一个数据结构来记录搜索树的构造过程。对树的每个结点:我们定义的数据结构包含四个元素：</h3><pre><code>1. n.State：对应的状态
2. n.Parent：搜索树产生该节点的节点
3. n.Action:父节点生成该节点时所采取的行动
4. n.Path-cost：代价用g（n）表示
</code></pre>
<p>结点是用来表示搜索树的数据结构。状态则对应于世界的一个配置情况最合适的数据结构是队列。### 3.3.2 问题求解算法的性能性能评估是四个方面：<br>    1. 完备性<br>    2. 最优性<br>    3. 时间复杂度<br>    4. 空间复杂度</p>
<p>评价搜索算法的有效性，我们可以只考虑搜索代价：通常取决于时间复杂度## 3.4 无信息搜索策略无信息搜索即盲目搜索，出了问题定义种提供的状态信息外没有任何附加信息。搜索算法要做的就是生成后继并区分目标装态与非目标状态知道一个非目标状态是否比其他状态更有希望接近目标的策略称为有信息搜索策略或者启发式搜索策略。### 3.4.1 宽度优先搜索先扩展根结点，接着扩展根结点的所有后继，然后再扩展它们的后继。每次都是拓展深度最浅的结点。如果路径代价是基于结点深度的非递减函数，宽度优先搜索是最优的但是坏处：时间和空间耗费不好。使用FIFO### 3.4.2 一致代价搜索当每一步的行动代价都相等时宽度优先搜索是最优的，因为它总是先扩展深度最浅的为扩展结点。一致代价搜索：我们可以找到一个对任何单步代价函数都是最优的算法。不再扩展深度最浅的结点，一致代价搜索两者不同：（一致代价搜索和宽度优先的差异）第一点是目标检测应用于结点被选择扩展时，而不是在节点生成（一致）第二个不同是如果边缘中的结点有更好的路径到达该结点那么会引入一个测试。（一致）和算法那个讲的一样### 3.4.3 深度优先搜索深度优先搜索（(kpth-first search) 总是扩展搜索树的当前边缘结点集中最深的结点。使用LIFO好一点优点：空间损耗的少深度优先搜索算法的效率严重依赖于使用的是图搜索还是树搜索。图搜索：有限状态空间是完备的，因为它至多扩展所有节点树搜索：则不完备，会陷入循环深度优先搜索的一种变形称为回溯搜索### 3.4.4 深度受限搜索深度界限的设定可以依据问题本身的知识在无线状态空间深度优先搜索会令人尴尬的失败，而这个问题可以通过对深度优先搜索设置界限l来避免。深度受限搜索可以通过修改一般的树搜索算法或者图搜索算法来实现或者作为简单递归算法来实现### 3.4.5 迭代加深的深度优先搜索经常和深度优先搜索结合确定最好的深度界限。做法：不断增大深度限制：从0到1到2.不断类推找到目标，当深度界限为d，即最浅的目标节点所在深度时，就能找到目标节点。每次迭代要把当前层的新结点全都探索过。结合一致代价搜索的迭代搜索是有价值的，在一致代价搜索确保最优化的同时避免了大量的内存需求。它的主要思想是用不断增加的路径代价界限代替不断增加的深度界限。基于这种思想的算法被称为迭代加长搜索（iterative lengthening search),### 3.4.6 双向搜索双向搜索：同时运行两个搜索：一个从初始状态向前搜索同时另一个从目标状态向后搜索，希望他们在中间某点相遇。实现：目标测试替换为检查两个方向的搜索的边缘结点集是否相交### 3.4.7 无信息搜索策略对比## 3.5 有信息的搜索策略有信息搜索：使用问题本身的定义之外的特定知识最佳优先搜索：基于评价函数被选择扩展的f由启发函数构成 h（n）是结点n到目标结点的最小代价路径的代价估计值### 3.5.1 贪婪最佳优先搜索贪婪最佳优先搜索：试图扩展离目标最近的结点还是按照启发函数给的值选最小的开始和上面那个一致有点像### 3.5.2 A<em>搜索：缩小总评估代价最佳优先搜索的最广为人知的形式称为A</em>搜索。它对结果的评估结合了g（n）。即到达此结点已经花费的代价同算法课保证最优的条件：可采纳性和一致性：保障的第一个条件是h（n）是一个可采纳启发式它不会过高估计到达目标的代价。第二个条件：略强于第一个的条件被称为一致性，即单调性。只作用在图搜索中使用A<em>算法。我们称启发式h（n）是一致的，如果对于每个节点n和通过任一行动a生成的n的每个后继节点n‘，从结点n到达目标的估计代价不大于从n到n’的单步代价与从n‘到目标的估计代价之和。缺点：费内存，在内存中保留了所有已生成的结点。### 3.5.3 存储受限的启发式搜索A</em>算法减少内存需求的简单办法就是将迭代加深的思想用在启发式搜索上。即迭代加深A<em>（IDA</em>）算法：主要区别是所用的截断值是f代价而不是搜索深度递归最佳优先搜索（RBFS)是一个简单的递归算法：它试图模仿标准的最佳优先搜索的操作但只是用线性的存储空间。它的结构和递归深度优先搜索类似，但是它不会不确定地沿着当前路径继续，它用变量&#x2F;<em>&#x2F;&#x2F;»»</em>&#x2F; 跟踪记录从当前结点的祖先可得到的最佳可选路径的&#x2F;值。如果当前结点超过了这个限制，递归将回到可选路径上。感觉还是跟那个最佳有点像 上课讲过的算法问题还是使用的内存过于笑了SMA<em>算法很像 A</em>算法，扩展最佳叶结点直到内存耗尽。就是说，要在搜索树中加入新结点就得抛弃一个旧结点。SMA<em>总是丢弃最差的叶结点 即&#x2F;值最高的结点。### 3.5.4 学习以促搜索Agent依赖于被称为元状态空间的重要概念，元状态空间中的每个状态都i要捕捉一个程序的内部状态，程序是在目标层状态空间种搜索。## 3.6 启发式函数### 3.6.1 启发式的精确度对性能的影响一种刻画启发式的方法是有效分支因子 b</em>，如果A<em>算法生成的总结点数为N，解得深度为d，那么b</em>就是深度为d的标准搜索树为了能够包括N+1节点所必需的粪质因子。公式### 3.6.2 从松弛问题出发设计可采纳的启发式减少了行动限制的问题称为松弛问题由于松弛问题增加了状态空间的边，原有问题中的任一最优解同样是松弛问题的最优解生成新的启发式函数的难点在于经常不能找到“无疑最好的”启发式。如果可采纳启发式的### 3.6.3 从子问题出发设计可采纳的启发式：模式数据库可采纳的启发式也可以从考虑给定问题的子问题的解代价得到模式数据库（pattern databases) 的思想就是对每个可能的子问题实例存储解代价两个子问题的代价之和仍然是求解整个问题的代价的下界。这就是不相交的模式数据库的思想### 3.6.4 从经验中学习启发式一个学习算法可以用来构造(够幸运的话）它能预测搜索过程中所出现的其他状态的解代价如果在状态描述外还能刻画给定状态的特征，归纳学习方法则是最可行的</p>
<h1 id="4-超越经典搜索"><a href="#4-超越经典搜索" class="headerlink" title="4 超越经典搜索"></a>4 超越经典搜索</h1><p>第三章中：环境是可观察的、确定的、已知的，问题解是一个行动序列。<br>第四章：前两节是状态空间的局部搜索算法，考虑对一个或多个状态进行评价和修改。<br>包括模拟退火法和遗传算法。<br>在后两节 不在强求环境的确定性和可观察性</p>
<h2 id="4-1-局部搜索算法和最优化问题"><a href="#4-1-局部搜索算法和最优化问题" class="headerlink" title="4.1 局部搜索算法和最优化问题"></a>4.1 局部搜索算法和最优化问题</h2><p>问题：到达目标的路径是不相关的，如过到目标的路径是无关紧要的，我们会考虑不同的算法<br>局部搜索算法从单个当前节点出发，只移动到它邻近状态，一般情况不保留搜索路径<br>优点：使用内存少 在系统化算法不适用的很大或无限的状态空间中找到合理的解</p>
<p>局部搜索算法对于解决纯粹的最优化问题十分有用</p>
<h3 id="4-1-1-爬山法"><a href="#4-1-1-爬山法" class="headerlink" title="4.1.1 爬山法"></a>4.1.1 爬山法</h3><p>爬山法：简单的循环过程，不断向值增加的方向持续移动。<br>爬山法有时被称为贪婪局部搜索<br>缺点<br>局部极大值：是一个比他的每个邻接结点都高的封顶，但是比全局最大值要小。会卡在局部最大值无路可走<br>山脊：山脊造成一系列的局部最大值，贪婪算法很难处理<br>高原：高原是在状态空间地形图上的一块平原区域，他可能是一块平的局部极大值，不存在上山的出口或者山肩。</p>
<p>对于遇到山肩，侧向移动是好主意，但是需要规定次数，不然会陷入死循环。</p>
<p>变形：<br>随机爬山法：在上山移动中随机的选择下一步<br>首选爬山法：实现了随机爬山法，随机的生成后继节点直到生成一个由于当前节点的后继。<br>随机重启爬山法：如果一开始没有成功，那么尝试，再尝试，通过随机生成初始状态来导引爬山法搜索，直到找到目标。</p>
<p>成功因素：依赖于状态空间地形图的性状：如果在图中几乎没有局部极大值和高原，随机重启爬山法会很快找到好的解。<br>许多实际问题的地形图就像平坦的地板上的一群变秃的箭猪，上面还有小的猪。</p>
<h3 id="4-1-2-模拟退火搜索"><a href="#4-1-2-模拟退火搜索" class="headerlink" title="4.1.2 模拟退火搜索"></a>4.1.2 模拟退火搜索</h3><p>爬山：会卡局部极大值，<br>纯粹的随机行走：效率低，从后继集合中完全等概率的随机选取后继。<br>模拟退火：结合爬山法和随机行走法<br>                内层循环与爬山法类似，只是没有选择最佳移动，选择的是随机移动，如果该移动使情况改善，该移动则被接受。否则，算法以某个小于1的概率接受该移动，如果移动导致状态变坏，概率则呈指数级下降导致评估值三角E变坏，这个概率也随温度T的降低而下降。</p>
<h3 id="4-1-3-局部束搜索"><a href="#4-1-3-局部束搜索" class="headerlink" title="4.1.3 局部束搜索"></a>4.1.3 局部束搜索</h3><p>内存总是有限的，但在内存中只保存一个结点又很极端。<br>局部束搜索算法记录k个状态而不是只记录一个，他从k个随机生成的扎UN国泰卡斯hi。每一步全部k个状态的所有后继状态全部被生成。如果有目标则停止。否则继续选择k个重复。</p>
<p>问题：如果k个状态缺乏多样性，很快会聚集到状态空间的一小块区域内，代价高。<br>解决：随机束搜索：是一种变形，并不是从候选后继集合中选择最好的k个后继状态，而是随机选择k个后继状态，其中选择给定后继状态的概率是状态值的递增函数。</p>
<h3 id="4-1-4-遗传算法"><a href="#4-1-4-遗传算法" class="headerlink" title="4.1.4 遗传算法"></a>4.1.4 遗传算法</h3><p>遗传算法（GA） 是随机束搜索的一个变形，通过把两个父状态结合来生成后继，而不是通过修改单一状态进行。这和随机剪枝搜集一样。</p>
<p>实现：也是从k个随机生成的状态开始，我们称之为种群，每个状态称为个体。用一个有限长度的字符串表示。 而每个状态都由它的目标函数或者说是适应度函数给出评估值。然后再字符串中随机选择一个位置作为杂交点，然后互相交换。两个父串杂交。</p>
<p>遗传算法理论用模式思想来解释运转过程，模式是指其中某些位未确定的子串。如果能匹配这个模式，那么称为实例。如果某模式实例的平均适应度超过均值，那么种群内这个模式的实例数量就会随时间增长。</p>
<h2 id="4-2-连续空间中的局部搜索"><a href="#4-2-连续空间中的局部搜索" class="headerlink" title="4.2 连续空间中的局部搜索"></a>4.2 连续空间中的局部搜索</h2><p>避免连续性问题的一种简单途径就是将每个状态的；邻接状态离散化。很多方法都试图利用地形图的梯度来找到最大值，目标函数的梯度是想来那个。<br>而且梯度表达式依赖于当前状态，这意味着我们只能局部的计算梯度。在给定梯度的局部正确表达式，可以通过下述公式更新当前状态来完成最陡上升爬山法。<br>x&lt;-x+a倒三角f（x）<br>其中a是很小的常数，为步长。<br>有很多调整a的不同方法，基本问题：a太小，需要太多步骤，a太大，搜索容易错过最大值。<br>线搜索技术试图通过扩展当前的梯度方向：通过反复使a加倍知道f开始再次下降。</p>
<p>对于许多问题最有效的算法是古老的newton-raphson方法。</p>
<p>局部搜索算法在连续状态空间和离散状态空间一样受到局部极大值、山脊、和高原的影响，可以使用随机重启和模拟退火算法</p>
<h2 id="4-3-使用不确定动作的搜索"><a href="#4-3-使用不确定动作的搜索" class="headerlink" title="4.3 使用不确定动作的搜索"></a>4.3 使用不确定动作的搜索</h2><p>如果环境不确定，感知信息告知Agent某一行动的结果倒地是什么，Agent的未来行动依赖于未来感知信息，所以问题的解不是一个序列，而是一个应急规划。</p>
<h3 id="4-3-1-不稳定的吸尘器世界"><a href="#4-3-1-不稳定的吸尘器世界" class="headerlink" title="4.3.1 不稳定的吸尘器世界"></a>4.3.1 不稳定的吸尘器世界</h3><h3 id="4-3-2-与或搜索树"><a href="#4-3-2-与或搜索树" class="headerlink" title="4.3.2 与或搜索树"></a>4.3.2 与或搜索树</h3><p>在确定性环境中，分支是由Agent在每个状态下的选择形成的，我们称这些节点为或节点。<br>                            再不确定环境 分支的形成可能是由于环境选择每个行动的后果，则为与节点。<br>算法并不检査当前状态是否是从根出发的其他路径上的重复状态，而这一点对效率来说相当重要</p>
<p>与或图同样可以用宽度优先或最佳优先方法搜索。启发式函数的概念必须修改为评估可能的解，而不是一个序列</p>
<h3 id="4-3-3-不断尝试"><a href="#4-3-3-不断尝试" class="headerlink" title="4.3.3 不断尝试"></a>4.3.3 不断尝试</h3><p>在规划中添加一个标签，之后就可以用这个标签而不用重复整个规划。</p>
<h2 id="4-4-使用部分可观察信息的搜索"><a href="#4-4-使用部分可观察信息的搜索" class="headerlink" title="4.4 使用部分可观察信息的搜索"></a>4.4 使用部分可观察信息的搜索</h2><p> 解决部分可观察问题的关键概念是信念状态</p>
<h3 id="4-4-1-无观察信息的搜索"><a href="#4-4-1-无观察信息的搜索" class="headerlink" title="4.4.1 无观察信息的搜索"></a>4.4.1 无观察信息的搜索</h3><p>如果Agent感知不到任何信息，我们称为无传感问题，有时候也成为了相容问题。</p>
<p>要求解无感知信息的问题，我们在信念状态空间中搜索而不是在实际状态空间中搜索</p>
<p>我们可以如下定义对应的无传感问题：<br>a. 信念状态：包含物理状态的每个可能集合。<br>b. 初始状态：显然是P中所有状态的集合<br>c. 行动：<br>d. 转移模型：Agent不知道信念状态中哪一个是对的，他知道在新年状态的某一物理状态采取某行动会导致另一状态。<br>行动后生成新的新年状态的过程称为预测<br>e. 目标测试 Agent需要一个确保生效的规划，意味着一个信念状态满足目标仅当其中所有的物理状态都满足Goal-test<br>f. 路径开销</p>
<h3 id="4-4-2-有观察信息的搜索"><a href="#4-4-2-有观察信息的搜索" class="headerlink" title="4.4.2 有观察信息的搜索"></a>4.4.2 有观察信息的搜索</h3><p>对于一般的部分可观察问题，我们需要规范环境如何生成 Agent 的感知信息。<br>我们可以将导致从一个信念状态转移到另一信念状态的一个特定行动看作是分为三个阶段发生：<br>预测阶段：<br>观察预测：<br>更新阶段：</p>
<h3 id="4-4-3-求解部分可观察环境中的问题"><a href="#4-4-3-求解部分可观察环境中的问题" class="headerlink" title="4.4.3 求解部分可观察环境中的问题"></a>4.4.3 求解部分可观察环境中的问题</h3><p>给出这样的形式化后，可以直接应用与或图搜索算法来求得解</p>
<h3 id="4-4-4-部分可观察环境中的Agent"><a href="#4-4-4-部分可观察环境中的Agent" class="headerlink" title="4.4.4 部分可观察环境中的Agent"></a>4.4.4 部分可观察环境中的Agent</h3><p>Agent 先对问题进行形式化，调用搜索算法（如与或图搜索）进行求解，然后执行解步骤<br>不同：1. 问题的解答是条件规划而不是一个序列。2. Agent需要在完成行动和接收感知信息时维护自身的信念状态。</p>
<h2 id="4-5-联机搜索Agent和位置环境"><a href="#4-5-联机搜索Agent和位置环境" class="headerlink" title="4.5 联机搜索Agent和位置环境"></a>4.5 联机搜索Agent和位置环境</h2><p>是 Agent 的脱机搜索算法。它们先对实际问题计算出完整的解决方案，然后再涉足现实世界执行解决方案</p>
<p>联机搜索1Agent 通过交替地计算和行动来完成任务：它先采取某个行动，然后观察环境变化并且计算出下一行动<br>因为联机搜索使得 Agent 可以将计算精力集中在实际发生的事件上，而不需要考虑那些也许会发生但很可能不会发生的事件</p>
<h3 id="4-5-1-联机搜索问题"><a href="#4-5-1-联机搜索问题" class="headerlink" title="4.5.1 联机搜索问题"></a>4.5.1 联机搜索问题</h3><p>联机搜索问题只能通过 Agent 执行行动来求解，它不是纯粹的计算过程<br>我们规定Agent只知道一下信息：<br>1.Action（s），返回s下可能进行的行动列表<br>2.单步耗散函数c（s，a，s‘） 注意知道结果为s‘才能用<br>3.Goal-test<br>需要注意的是，Agent 并不知道 RESULT（s，a)的值，</p>
<h3 id="4-5-2-联机搜索Agent"><a href="#4-5-2-联机搜索Agent" class="headerlink" title="4.5.2 联机搜索Agent"></a>4.5.2 联机搜索Agent</h3><h3 id="4-5-3-联机局部搜索"><a href="#4-5-3-联机局部搜索" class="headerlink" title="4.5.3 联机局部搜索"></a>4.5.3 联机局部搜索</h3><p>、爬山也是联机搜索算法<br>随机行走来取代随机重启，以探索环境。<br>随机行走是指在当前状态下随机选择可能的行动之一；选择的时候可以偏向选择那些尚未尝试过的行动。</p>
<h3 id="4-5-4-联机搜索中的学习"><a href="#4-5-4-联机搜索中的学习" class="headerlink" title="4.5.4 联机搜索中的学习"></a>4.5.4 联机搜索中的学习</h3><p>联机搜索 Agent 初始对环境的无知提供了一些学习的机会<br>局部搜索 Agent<br>利用局部更新规则可以得到每个状态更精确的估计值</p>
<h2 id="5-对抗搜索"><a href="#5-对抗搜索" class="headerlink" title="5 对抗搜索"></a>5 对抗搜索</h2><h3 id="5-1-博弈"><a href="#5-1-博弈" class="headerlink" title="5. 1 博弈"></a>5. 1 博弈</h3><pre><code>讨论的是竞争环境 ，竞争环境中每个Agent的目标之间是有冲突的，这就引出了对抗搜索问题。（博弈）
</code></pre>
<p>博弈的组成：</p>
<p>零和博弈：常量和一样</p>
<p>###5.2 博弈中的优化策略<br>在一般搜索问题中，最优解是到达目标状态的一系列行动棗 终止状态即为取胜</p>
<p>###5.2.1极大极小算法<br>极小极大算法（图 5.3) 从当前状态计算极小极大决策。它使用了简单的递归算法计算<br>每个后继的极小极大值，<br>跟算法里面那个有点像，决策树</p>
<h2 id="5-2。2-多人博弈时的最优决策"><a href="#5-2。2-多人博弈时的最优决策" class="headerlink" title="5.2。2 多人博弈时的最优决策"></a>5.2。2 多人博弈时的最优决策</h2><p>效益函数返回的应该是一个像两只</p>
<h2 id="5-3-剪枝问题"><a href="#5-3-剪枝问题" class="headerlink" title="5.3 剪枝问题"></a>5.3 剪枝问题</h2><p>尽可能消除部分搜索树<br>就是不符合范围的就删了</p>
<h3 id="5-3-1-行棋排序"><a href="#5-3-1-行棋排序" class="headerlink" title="5.3.1 行棋排序"></a>5.3.1 行棋排序</h3><p>应该先找最优秀的后继 那样可以排除分支节点</p>
<p>增加动态行棋排序方案，如先试图采用以前走过的最好行棋，可能让我们非常接近理<br>论极限</p>
<p>当结果一样 ，就不用计算两次，可以直接用表<br>存储以前见过的棋局的哈希表一般被称为换位表；<br>它本质上和图搜索中的 explored 表相同（参见 3.3 节)。使用换位表可以取得很好的动态效<br>果</p>
<h2 id="5-4-不完美的实时决策"><a href="#5-4-不完美的实时决策" class="headerlink" title="5.4 不完美的实时决策"></a>5.4 不完美的实时决策</h2><p>就是一般来讲 算法要搜索到中止状态，但是我们的实际情况呢，不太可能全搜完在决策，所以我们是实时的，要引入评估函数EVAL用于搜索，用启发式评估函数取代效用函数，用决策什么时候运用EVal的阶段测试取代终止测试。</p>
<h3 id="5-4-1"><a href="#5-4-1" class="headerlink" title="5.4.1"></a>5.4.1</h3><p>以前的评估函数是到目标点的距离，国际象棋的评估函数需要考虑的很多<br>这张就是主要介绍一下如何评估</p>
<h3 id="5-4-2-截断搜索"><a href="#5-4-2-截断搜索" class="headerlink" title="5.4.2 截断搜索"></a>5.4.2 截断搜索</h3><p>下棋会有谁吃谁 不是很平静的吃，所以评估值会发生很大的变化，评估函数只适用于那些静态棋局，非静态棋局可以进一步扩展直到变为静态棋局。这种额外的搜索称为静态搜索；有时它只考虑某些类型的棋招，诸如吃子能够快速消解棋局的不确定性。</p>
<p>地平线效应更难消除。这是指对手招数导致我方严重损失并且从理论上基本无法避免<br>时。</p>
<p>单步延伸是避免地平线效应的一种策略，单步延伸指的是在给定棋局中一种棋招要“明<br>显好于”其他棋招。<br>截断搜索思想<br>在α-β剪枝算法中，将Terminal-test 被替换程cutoff-test(state,depth)，Utility被替换程eval(state)。<br>cutoff-test(state,depth)截断策略：<br>当大于固定深度时返回True；<br>根据游戏允许的时间来决定深度。特点<br>评估函数的近似性会使截断搜索可能导致错误；<br>评估函数只适应于静态棋局，即不会很快出现大摇摆的棋局。<br>难以消除地平线效应（对方招数导致我方严重损失并且理论上基本无法避免），固定深度搜索会相信这些延缓招数能阻止实际无法避免的困境。<br>【单步延伸策略可以用来避免地平线效应，发现并记住一种“明显好于其他招数”的招数，当搜索达到指定深度界限，若单步延伸合法，则考虑此招。由于单步延伸很少，所以不会增加太多开销】</p>
<h3 id="5-4-3-向前剪枝"><a href="#5-4-3-向前剪枝" class="headerlink" title="5.4.3 向前剪枝"></a>5.4.3 向前剪枝</h3><p>前向剪枝，是指在某个结点上无需进一步考虑而直接剪枝一些子结点<br>方法：柱搜索：在每一层，只考虑最好的n步行棋可能，这称为“柱”，并不是考虑所有行棋招数</p>
<h3 id="5-4-4-搜索与查表搜索与查表"><a href="#5-4-4-搜索与查表搜索与查表" class="headerlink" title="5.4.4 搜索与查表搜索与查表"></a>5.4.4 搜索与查表搜索与查表</h3><p>开局时的行棋大多依赖于人类的专业知识；<br>接近尾声的棋局可能性有限；<br>在开局和尾声阶段可以通过查表的方式来进行行棋。</p>
<h3 id="5-5-随机博弈"><a href="#5-5-随机博弈" class="headerlink" title="5.5 随机博弈"></a>5.5 随机博弈</h3><p>局面没有明确的极大极小值，只能计算棋局的期望值：机会结点所有可能结果的平均值。<br>将确定性博弈中的极小极大值一般化为包含机会结点的博弈的期望极大极小值。</p>
<p>###5.5.1 评估函数<br>评估函数应该与棋局获胜的概率成线性变换</p>
<p>#5.6 五、部分可观察的博弈军旗<br>棋子可以移动但对方看不见棋子是什么<br>使用信念状态牌类<br>随机部分可观察<br>需要概率推算来制定决策</p>
<p>#6。 定义约束问题</p>
<h2 id="6-1"><a href="#6-1" class="headerlink" title="6.1"></a>6.1</h2><p>三个成分：<br>X变量集合<br>D是值域集合<br>C是描述变量取值的约束集合</p>
<h3 id="6-1-1-实例：地图着色问题"><a href="#6-1-1-实例：地图着色问题" class="headerlink" title="6.1.1 实例：地图着色问题"></a>6.1.1 实例：地图着色问题</h3><p>邻接的不能同一种颜色</p>
<h3 id="6-1-2-作业调度问题："><a href="#6-1-2-作业调度问题：" class="headerlink" title="6.1.2 作业调度问题："></a>6.1.2 作业调度问题：</h3><h3 id="6-1-3-CSP的形式化"><a href="#6-1-3-CSP的形式化" class="headerlink" title="6.1.3 CSP的形式化"></a>6.1.3 CSP的形式化</h3><p>最简单的 CSP 是指涉及的变量是离散的有限值域的</p>
<p>如果离散值域是无限的，不太可能枚举全部的情况，就只能用约束语言替代了，<br>后面的可以看<br>第6章 约束满足问题CSP </p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> </li>
        <li><strong>Author:</strong> 杨小鹤</li>
        <li><strong>Created at:</strong> 2023-08-09 02:07:04</li>
        
            <li>
                <strong>Updated at:</strong> 2023-08-09 02:06:50
            </li>
        
        <li>
            <strong>Link:</strong> https://redefine.ohevan.com/2023/08/09/ai/
        </li>
        <li>
            <strong>License:</strong> This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
        </li>
    </ul>
</div>

            </div>
        

        

        

        
            <div class="article-nav">
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/2023/08/08/html/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">HTML（HyperText Markup Language）</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container">
                <div class="comments-container pjax">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-swup-reload-script>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title"></div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-1-1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%80%BB%E8%BF%B0%EF%BC%9A%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83-%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E8%A1%8C%E5%8A%A8-%E5%90%88%E7%90%86%E7%9A%84%E6%80%9D%E8%80%83-%E5%90%88%E7%90%86%E5%9C%B0%E8%A1%8C%E5%8A%A8-1-1-1-%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E8%A1%8C%E5%8A%A8%EF%BC%9A%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95%E7%9A%84%E9%80%94%E5%BE%84%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%9A%E4%B8%BA%E6%99%BA%E8%83%BD%E6%8F%90%E4%BE%9B%E4%B8%80%E4%B8%AA%E4%BB%A4%E4%BA%BA%E6%BB%A1%E6%84%8F%E7%9A%84%E5%8F%AF%E6%93%8D%E4%BD%9C%E7%9A%84%E5%AE%9A%E4%B9%89%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%9C%80%E8%A6%81%E5%85%B7%E6%9C%89%E7%9A%84%E8%83%BD%E5%8A%9B%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%81%E7%9F%A5%E8%AF%86%E8%A1%A8%E8%BE%BE%E3%80%81%E8%87%AA%E5%8A%A8%E6%8E%A8%E7%90%86%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%8C%E5%85%A8%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95%EF%BC%9A%E5%8C%85%E6%8B%AC%E8%A7%86%E9%A2%91%E4%BF%A1%E5%8F%B7%E4%BB%A5%E4%BE%BF%E8%AE%AF%E9%97%AE%E8%80%85%E5%8D%B3%E5%8F%AF%E6%B5%8B%E8%AF%95%E5%AF%B9%E6%96%B9%E7%9A%84%E6%84%9F%E7%9F%A5%E8%83%BD%E5%8A%9B%EF%BC%8C%E5%8F%88%E6%9C%89%E6%9C%BA%E4%BC%9A%E9%80%9A%E8%BF%87%E8%88%B1%E5%8F%A3%E8%88%B9%E4%BD%93%E7%89%A9%E7%90%86%E5%AF%B9%E8%B1%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9C%A8%E4%BB%A5%E4%B8%8A%E5%9B%9B%E7%A7%8D%E8%BF%98%E9%9C%80%E8%A6%81%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E3%80%81%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E4%B9%A0%E4%BB%A5%E9%80%9A%E8%BF%87%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%80%8C%E7%A0%94%E7%A9%B6AI%E4%B8%8D%E6%98%AF%E4%B8%80%E5%AE%9A%E7%9A%84%EF%BC%8C%E4%BE%8B%E5%A6%82%E8%88%AA%E7%A9%BA%E5%B7%A5%E7%A8%8B%E4%B8%8D%E4%BC%9A%E5%90%A7%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%B9%89%E4%B8%BA%E5%88%B6%E9%80%A0%E4%B8%BA%E8%83%BD%E5%AE%8C%E5%85%A8%E5%83%8F%E9%B8%BD%E5%AD%90%E4%B8%80%E6%A0%B7%E9%A3%9E%E8%A1%8C%E7%9A%84%E6%9C%BA%E5%99%A8%EF%BC%8C%E4%BB%A5%E8%87%B3%E4%BB%96%E4%BB%AC%E5%8F%AF%E4%BB%A5%E9%AA%97%E8%BF%87%E5%85%B6%E4%BB%96%E9%B8%BD%E5%AD%90-1-1-2-%E5%83%8F%E4%BA%BA%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83%EF%BC%9A%E8%AE%A4%E7%9F%A5%E5%BB%BA%E6%A8%A1%E7%9A%84%E9%80%94%E5%BE%84%E5%89%8D%E6%8F%90%E7%A1%AE%E5%AE%9A%E4%BA%BA%E6%98%AF%E5%A6%82%E4%BD%95%E6%80%9D%E8%80%83%E7%9A%84%EF%BC%9A-%E9%80%9A%E8%BF%87%E5%86%85%E7%9C%81%EF%BC%88%E6%8D%95%E8%8E%B7%E6%88%91%E4%BB%AC%E7%9A%84%E6%80%9D%E7%BB%B4%E8%BF%87%E7%A8%8B%EF%BC%89-%E9%80%9A%E8%BF%87%E5%BF%83%E7%90%86%E5%AE%9E%E9%AA%8C%EF%BC%88%E8%A7%82%E5%AF%9F%E5%B7%A5%E4%BD%9C%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BA%BA%EF%BC%89-%E9%80%9A%E8%BF%87%E8%84%91%E6%88%90%E5%83%8F%EF%BC%88%E8%A7%82%E5%AF%9F%E5%B7%A5%E4%BD%9C%E4%B8%AD%E7%9A%84%E5%A4%B4%E8%84%91%EF%BC%89%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6%EF%BC%88cognitive-science-%E8%BF%99%E4%B8%AA%E4%BA%A4%E5%8F%89%E5%AD%A6%E7%A7%91%E9%A2%86%E5%9F%9F%E6%8A%8A%E6%9D%A5%E8%87%AA-AI%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9D%A5%E8%87%AA%E5%BF%83%E7%90%86%E5%AD%A6%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%8A%80%E6%9C%AF%E7%9B%B8%E7%BB%93%E5%90%88%E8%AF%95%E5%9B%BE%E6%9E%84%E5%BB%BA%E4%B8%80%E7%A7%8D%E7%B2%BE%E7%A1%AE%E4%B8%94%E5%8F%AF%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BA%BA%E7%B1%BB%E6%80%9D%E7%BB%B4%E7%90%86%E8%AE%BA-1-1-3-%E5%90%88%E7%90%86%E7%9A%84%E6%80%9D%E8%80%83%EF%BC%9A%E6%80%9D%E7%BB%B4%E6%B3%95%E5%88%99%E7%9A%84%E9%80%94%E5%BE%84%E4%B8%89%E6%AE%B5%E8%AE%BA%EF%BC%88%E4%BA%9A%E9%87%8C%E5%A3%AB%E5%A4%9A%E5%BE%B7%EF%BC%89%EF%BC%9A%E5%9C%A8%E7%BB%99%E5%AE%9A%E6%AD%A3%E7%A1%AE%E5%89%8D%E6%8F%90%E6%97%B6%E6%80%BB%E4%BA%A7%E7%94%9F%E6%AD%A3%E7%A1%AE%E7%BB%93%E8%AE%BA%E7%9A%84%E8%AE%BA%E8%AF%81%E7%BB%93%E6%9E%84%E6%8F%90%E4%BE%9B%E4%BA%86%E6%A8%A1%E5%BC%8F%E3%80%82-%E5%BC%80%E5%88%9B%E4%BA%86%E9%80%BB%E8%BE%91%E5%AD%A6%E5%9B%A0%E6%AD%A4%E4%BA%A7%E7%94%9F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%AD%E7%9A%84%E9%80%BB%E8%BE%91%E4%B8%BB%E4%B9%89%EF%BC%9A%E5%B8%8C%E6%9C%9B%E8%83%BD%E7%94%A8%E9%80%BB%E8%BE%91%E6%9D%A5%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%EF%BC%8C%E7%BB%99%E4%B8%96%E4%B8%8A%E5%90%84%E4%B8%AA%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E9%99%88%E8%BF%B0%E5%88%B6%E5%AE%9A%E4%B8%80%E7%A7%8D%E7%B2%BE%E7%A1%AE%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%B3%95-1-1-4-%E5%90%88%E7%90%86%E5%9C%B0%E8%A1%8C%E5%8A%A8%EF%BC%9A%E5%90%88%E7%90%86agent%E7%9A%84%E9%80%94%E5%BE%84Agent%EF%BC%9A%E8%83%BD%E5%A4%9F%E8%A1%8C%E5%8A%A8%E7%9A%84%E6%9F%90%E7%A7%8D%E4%B8%9C%E8%A5%BFRational-Agent%EF%BC%9A%E5%90%88%E7%90%86Agent%E6%98%AF%E4%B8%80%E4%B8%AA%E4%B8%BA%E4%BA%86%E5%AE%9E%E7%8E%B0%E6%9C%80%E4%BD%B3%E7%BB%93%E6%9E%9C%EF%BC%8C%E6%88%96%E8%80%85%E5%BD%93%E5%AD%98%E5%9C%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%97%B6%EF%BC%8C%E4%B8%BA%E4%BA%86%E5%AE%9E%E7%8E%B0%E6%9C%80%E4%BD%B3%E6%9C%9F%E6%9C%9B%E7%BB%93%E6%9E%9C%E8%80%8C%E8%A1%8C%E5%8A%A8%E7%9A%84agent%E3%80%82%E5%90%88%E7%90%86agent%E7%9A%84%E9%80%94%E5%BE%84%E4%B8%8E%E5%85%B6%E4%BB%96%E9%80%94%E5%BE%84%E7%9B%B8%E6%AF%94%E6%9C%89%E4%B8%A4%E4%B8%AA%E4%BC%98%E7%82%B9%EF%BC%9A%E5%9B%A0%E4%B8%BA%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%8E%A8%E7%90%86%E5%8F%AA%E6%98%AF%E5%AE%9E%E7%8E%B0%E5%90%88%E7%90%86%E6%80%A7%E7%9A%84%E5%87%A0%E7%A7%8D%E5%8F%AF%E8%83%BD%E7%9A%84%E6%9C%BA%E5%88%B6%E4%B9%8B%E4%B8%80%E4%B8%94%E6%9B%B4%E7%BB%8F%E5%BE%97%E8%B5%B7%E6%A3%80%E9%AA%8C%E3%80%82-1-2-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%9F%BA%E7%A1%80-1-2-1-%E5%93%B2%E5%AD%A6%E5%A4%A7%E8%87%B4%E5%8E%86%E5%8F%B2%E7%9A%84%E4%BA%9A%E9%87%8C%E5%A3%AB%E5%A4%9A%E5%BE%B7%EF%BC%9A%E9%98%90%E8%BF%B0%E6%94%AF%E9%85%8D%E5%A4%B4%E8%84%91%E7%90%86%E6%80%A7%E9%83%A8%E5%88%86%E7%9A%84%E4%B8%80%E7%BB%84%E7%B2%BE%E7%A1%AE%E8%A7%84%E5%88%99%E7%9A%84%E4%BA%BA%E3%80%82Ramon-Lull%EF%BC%9A%E6%9C%89%E7%94%A8%E7%9A%84%E6%8E%A8%E7%90%86%E5%8F%AF%E4%BB%A5%E7%94%A8%E6%9C%BA%E6%A2%B0%E4%BA%BA%E9%80%A0%E7%89%A9%E5%AE%9E%E7%8E%B0Thomas-Hobbes%EF%BC%9A%E6%8F%90%E5%87%BA%E6%8E%A8%E7%90%86%E5%B0%B1%E5%83%8F%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E8%BE%BE%E8%8A%AC%E5%A5%87%EF%BC%9A%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%80%E5%8F%B0%E6%9C%BA%E6%A2%B0%E8%AE%A1%E7%AE%97%E5%99%A8%E2%80%A6%E2%80%A6%E7%AC%9B%E5%8D%A1%E5%B0%94%E6%98%AF%E4%BA%8C%E5%85%83%E8%AE%BA%E7%9A%84%E6%94%AF%E6%8C%81%E8%80%85%EF%BC%9A%E4%BB%96%E8%AE%A4%E4%B8%BA%E4%BA%BA%E7%B1%BB%E5%A4%B4%E8%84%91%E5%AD%98%E5%9C%A8%E4%B8%80%E9%83%A8%E5%88%86%E5%9C%A8%E8%87%AA%E7%84%B6%E4%B9%8B%E5%A4%96%E4%B8%8D%E5%8F%97%E7%89%A9%E7%90%86%E5%AE%9A%E5%BE%8B%E6%94%AF%E9%85%8D%E7%9A%84%E4%B8%9C%E8%A5%BF-%E5%8A%A8%E7%89%A9%E6%97%A0%E4%B9%8B%E5%90%8E%E5%AF%B9%E4%BA%8C%E5%85%83%E8%AE%BA%E7%9A%84%E6%9B%BF%E6%8D%A2%E7%89%A9%E6%98%AF%E5%94%AF%E7%89%A9%E4%B8%BB%E4%B9%89%EF%BC%88materialism%EF%BC%89%E7%BB%8F%E9%AA%8C%E4%B8%BB%E4%B9%89%E5%A7%8B%E4%BA%8E%E5%9F%B9%E6%A0%B9-%E7%84%B6%E5%90%8E%E5%86%8D%E5%A4%A7%E5%8D%AB%E4%BC%91%E8%B0%9F%E6%8F%90%E5%87%BA%E4%BA%86%E5%BD%92%E7%BA%B3%E5%8E%9F%E7%90%86-%EF%BC%9A%E4%B8%80%E8%88%AC%E8%A7%84%E5%88%99%E9%80%9A%E8%BF%87%E6%8F%AD%E7%A4%BA%E8%A7%84%E5%88%99%E4%B8%AD%E5%85%83%E7%B4%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E9%87%8D%E5%A4%8D%E5%85%B3%E8%81%94%E6%9D%A5%E8%8E%B7%E5%BE%97%E9%80%BB%E8%BE%91%E5%AE%9E%E8%AF%81%E4%B8%BB%E4%B9%89%EF%BC%9A%E6%89%80%E6%9C%89%E7%9F%A5%E8%AF%86%E9%83%BD%E5%8F%AF%E7%94%A8%E6%9C%80%E7%BB%88%E4%B8%8E%E5%AF%B9%E5%BA%94%E4%BA%8E%E6%84%9F%E7%9F%A5%E8%BE%93%E5%85%A5%E7%9A%84%E8%A7%82%E5%AF%9F%E8%AF%AD%E5%8F%A5%E7%9B%B8%E8%81%94%E7%B3%BB%E7%9A%84%E9%80%BB%E8%BE%91%E7%90%86%E8%AE%BA%E6%9D%A5%E5%88%BB%E7%94%BB%E3%80%82%E5%A4%B4%E8%84%91%E7%9A%84%E5%93%B2%E5%AD%A6%E6%8F%8F%E8%BF%B0%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E5%85%83%E7%B4%A0%E6%98%AF%E7%9F%A5%E8%AF%86%E4%B8%8E%E8%A1%8C%E5%8A%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E8%81%94%E7%B3%BB%E3%80%82%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98%E5%AF%B9%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%98%AF%E6%9E%81%E5%85%B6%E9%87%8D%E8%A6%81%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%99%BA%E8%83%BD%E6%97%A2%E8%A6%81%E6%B1%82%E6%8E%A8%E7%90%86%E5%8F%88%E8%A6%81%E6%B1%82%E8%A1%8C%E5%8A%A8%E3%80%82%E4%BA%9A%E9%87%8C%E5%A3%AB%E5%A4%9A%E5%BE%B7%E6%8F%90%E5%87%BA%EF%BC%9A%E9%80%9A%E8%BF%87%E7%9B%AE%E6%A0%87%E4%B8%8E%E8%A1%8C%E5%8A%A8%E7%BB%93%E6%9E%9C%E7%9A%84%E7%9F%A5%E8%AF%86%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%BB%E8%BE%91%E5%85%B3%E7%B3%BB%E6%9D%A5%E8%AF%81%E6%98%8E%E8%A1%8C%E5%8A%A8%E6%98%AF%E6%AD%A3%E5%BD%93%E7%9A%84%E3%80%82%E5%90%8E%E4%BA%BA%E5%88%A9%E7%94%A8%E8%BF%99%E7%A7%8D%E7%AE%97%E6%B3%95%E6%88%90%E7%AB%8B%E4%BA%86%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1-2-2-%E6%95%B0%E5%AD%A6%E4%B8%89%E4%B8%AA%E9%A2%86%E5%9F%9F%EF%BC%9A%E9%80%BB%E8%BE%91-%E8%AE%A1%E7%AE%97%E5%92%8C%E6%A6%82%E7%8E%87%E9%80%BB%E8%BE%91%E4%B8%8A%EF%BC%9A%E5%B8%83%E5%B0%94%E9%80%BB%E8%BE%91-%E5%BC%97%E9%9B%B7%E6%A0%BC%E6%89%A9%E5%B1%95%E4%BA%86%E5%B8%83%E5%B0%94%E9%80%BB%E8%BE%91%E8%BF%99%E4%BE%BF%E6%98%AF%E7%8E%B0%E5%9C%A8%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E9%98%B6%E9%80%BB%E8%BE%91%E3%80%82%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%B9%B3%E5%87%A1%E7%AE%97%E6%B3%95%EF%BC%9A%E6%9C%80%E5%A4%A7%E5%85%AC%E7%BA%A6%E6%95%B0-%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E5%93%A5%E5%BE%B7%E5%B0%94%E7%9A%84%E4%B8%8D%E5%AE%8C%E5%A4%87%E6%80%A7%E5%AE%9A%E7%90%86%EF%BC%9A%E4%B8%80%E6%A0%B7%E5%BC%BA%E7%9A%84%E4%BB%BB%E4%BD%95%E5%BD%A2%E5%BC%8F%E7%90%86%E8%AE%BA%E4%B8%AD%E9%83%BD%E5%AD%98%E5%9C%A8%E4%B8%8D%E5%8F%AF%E5%88%A4%E6%96%AD%E7%9A%84%E7%9C%9F%E8%AF%AD%E5%8F%A5%E4%BD%86%E6%98%AF%E6%9C%89%E5%BE%88%E5%A4%9A%E9%97%AE%E9%A2%98%E6%98%AF%E4%B8%8D%E5%8F%AF%E5%8F%8A%E7%AE%97%E7%9A%84%E6%98%93%E5%A4%84%E7%90%86%E6%80%A7%E7%9A%84%E6%A6%82%E5%BF%B5%E5%85%B7%E6%9C%89%E6%9B%B4%E5%A4%A7%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%9A%E7%9B%B8%E6%AF%94%E4%BA%8E%E5%8F%AF%E5%88%A4%E5%AE%9A%E6%80%A7%E5%92%8C%E5%8F%AF%E8%AE%A1%E7%AE%97%E6%80%A7%E4%B8%8D%E6%98%93%E5%A4%84%E7%90%86%EF%BC%9A%E5%A6%82%E6%9E%9C%E8%A7%A3%E5%86%B3%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E7%9A%84%E5%AE%9E%E4%BE%8B%E6%89%80%E9%9C%80%E6%97%B6%E9%97%B4%E9%9A%8F%E5%AE%9E%E4%BE%8B%E7%9A%84%E8%A7%84%E6%A8%A1%E6%88%90%E6%8C%87%E6%95%B0%E7%BA%A7%E5%9C%B0%E5%A2%9E%E9%95%BF%EF%BC%8C%E9%82%A3%E4%B9%88%E8%AF%A5%E9%97%AE%E9%A2%98%E7%A7%B0%E4%B8%BA%E4%B8%8D%E6%98%93%E5%A4%84%E7%90%86%E7%9A%84%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9A%E4%B8%8D%E6%98%93%E5%A4%84%E7%90%86%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A%E6%96%AF%E8%92%82%E6%96%87%E5%BA%93%E5%85%8B%E5%92%8C%E5%8D%A1%E6%99%AE%EF%BC%9ANP-completeness-%E8%AF%81%E6%98%8E%E4%BA%86%E5%AD%98%E5%9C%A8%E5%A4%A7%E9%87%8F%E7%BB%8F%E5%85%B8%E7%BB%84%E5%90%88%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%8E%A8%E7%90%86%E9%97%AE%E9%A2%98%E6%98%AF-NP-%E5%AE%8C%E5%85%A8%E7%9A%84%E3%80%82NP-%E5%AE%8C%E5%85%A8%E9%97%AE%E9%A2%98%E7%B1%BB%E5%8F%AF%E5%BD%92%E7%BA%A6%E5%88%B0%E7%9A%84%E4%BB%BB%E4%BD%95%E9%97%AE%E9%A2%98%E7%B1%BB%E5%8F%AF%E8%83%BD%E5%B0%B1%E6%98%AF%E4%B8%8D%E6%98%93%E5%A4%84%E7%90%86%E7%9A%84%EF%BC%88%E4%B8%80%E8%88%AC%E8%AE%A4%E4%B8%BAnp%E5%AE%8C%E5%85%A8%E6%98%AF%E4%B8%8D%E6%98%93%E5%A4%84%E7%90%86%E7%9A%84%EF%BC%89%E6%A6%82%E7%8E%87%EF%BC%9A%E6%8C%89%E7%85%A7%E8%B5%8C%E5%8D%9A%E4%BA%8B%E4%BB%B6%E7%9A%84%E5%8F%AF%E8%83%BD%E7%BB%93%E6%9E%9C%E6%9D%A5%E6%8F%8F%E8%BF%B0%E5%AE%83%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E8%A7%84%E5%88%99%E6%9E%84%E6%88%90%E4%BA%86%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%A4%A7%E5%A4%9A%E6%95%B0%E7%94%A8%E4%BA%8E%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%8E%A8%E7%90%86%E7%9A%84%E7%8E%B0%E4%BB%A3%E6%96%B9%E6%B3%95%E7%9A%84%E5%9F%BA%E7%A1%80-1-2-3-%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA%EF%BC%9A-%E6%8A%8A%E6%A6%82%E7%8E%87%E7%90%86%E8%AE%BA%E5%92%8C%E6%95%88%E7%94%A8%E7%90%86%E8%AE%BA%E7%BB%93%E5%90%88%E8%B5%B7%E6%9D%A5%EF%BC%8C%E4%B8%BA%E5%9C%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%81%9A%E5%87%BA%E5%86%B3%E7%AD%96%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BD%A2%E5%BC%8F%E5%8C%96%E4%B8%94%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82-1-2-4-%E7%A5%9E%E7%BB%8F%E7%A7%91%E5%AD%A6-1-2-5-%E5%BF%83%E7%90%86%E5%AD%A6-1-2-6-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%B7%A5%E7%A8%8B-%E4%BA%8C-%E6%99%BA%E8%83%BDagent-2-1-agent-%E5%92%8C%E7%8E%AF%E5%A2%83agent-%E6%8E%A5%E5%8F%97%E9%94%AE%E7%9B%98%E6%95%B2%E5%87%BB%E3%80%81%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%92%8C%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%8C%85%E4%BD%9C%E4%B8%BA%E4%BC%A0%E6%84%9F%E5%99%A8%E8%BE%93%E5%85%A5Agent-%E7%9A%84%E6%84%9F%E7%9F%A5%E5%BA%8F%E5%88%97%E6%98%AF%E8%AF%A5-Agent-%E6%89%80%E6%94%B6%E5%88%B0%E7%9A%84%E6%89%80%E6%9C%89%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AE%8C%E6%95%B4%E5%8E%86%E5%8F%B2%E3%80%82Agent-%E5%87%BD%E6%95%B0%E6%8F%8F%E8%BF%B0%E4%BA%86-Agent-%E7%9A%84%E8%A1%8C%E4%B8%BA%EF%BC%8C%E5%AE%83%E5%B0%86%E4%BB%BB%E6%84%8F%E7%BB%99%E5%AE%9A%E6%84%9F%E7%9F%A5%E5%BA%8F%E5%88%97%E6%98%A0%E5%B0%84%E4%B8%BA%E8%A1%8C%E5%8A%A8Agent-%E7%A8%8B%E5%BA%8F%E5%88%99%E6%98%AF%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%EF%BC%8C%E5%AE%83%E5%9C%A8%E4%B8%80%E4%BA%9B%E7%89%A9%E7%90%86%E7%B3%BB%E7%BB%9F%E5%86%85%E9%83%A8%E8%BF%90%E8%A1%8C%E3%80%82%E5%B0%B1%E6%98%AF%E5%87%BD%E6%95%B0%E6%98%AF%E8%A1%A8%E8%BE%BE%EF%BC%8C%E7%A8%8B%E5%BA%8F%E6%98%AF%E6%89%A7%E8%A1%8C-2-2-%E5%A5%BD%E7%9A%84%E8%A1%8C%E4%B8%BA%EF%BC%9A%E7%90%86%E6%80%A7%E7%9A%84%E6%A6%82%E5%BF%B5%E7%90%86%E6%80%A7Agent%E6%98%AF%E5%81%9A%E4%BA%8B%E6%AD%A3%E7%A1%AE%E7%9A%84Agent%E3%80%82%E6%B8%B4%E6%9C%9B%E5%88%A9%E7%94%A8%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E8%A1%A8%E8%BF%B0%EF%BC%8C%E5%AE%83%E5%AF%B9%E7%8E%AF%E5%A2%83%E7%8A%B6%E6%80%81%E7%9A%84%E4%BB%BB%E4%BD%95%E7%BB%99%E5%AE%9A%E5%BA%8F%E5%88%97%E8%BF%9B%E8%A1%8C%E8%AF%84%E4%BC%B0%E3%80%82%E4%BD%9C%E4%B8%BA%E4%B8%80%E8%88%AC%E5%8E%9F%E5%88%99%EF%BC%8C%E6%9C%80%E5%A5%BD%E6%A0%B9%E6%8D%AE%E5%AE%9E%E9%99%85%E5%9C%A8%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%B8%8C%E6%9C%9B%E5%BE%97%E5%88%B0%E7%9A%84%E7%BB%93%E6%9E%9C%E6%9D%A5%E8%AE%BE%E8%AE%A1%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E6%A0%B9%E6%8D%AE-Agent-%E8%A1%A8%E7%8E%B0%E5%87%BA%E7%9A%84%E8%A1%8C%E4%B8%BA%E3%80%82-2-2-1-%E7%90%86%E6%80%A7%E7%90%86%E6%80%A7%E5%88%A4%E6%96%AD%E7%9A%84%E5%9B%9B%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%AE%9A%E4%B9%89%E6%88%90%E5%8A%9F%E6%A0%87%E5%87%86%E7%9A%84%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-Agent%E5%AF%B9%E7%8E%AF%E5%A2%83%E7%9A%84%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86-Agent%E5%8F%AF%E4%BB%A5%E5%AE%8C%E6%88%90%E7%9A%84%E8%A1%8C%E5%8A%A8-Agent%E6%88%AA%E6%AD%A2%E5%88%B0%E6%AD%A4%E6%97%B6%E7%9A%84%E6%84%9F%E7%9F%A5%E5%BA%8F%E5%88%97%E4%BB%8E%E6%9D%A5%E5%BE%97%E5%88%B0%E7%90%86%E6%80%A7%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%9A%E5%AF%B9%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E6%84%9F%E7%9F%A5%E5%BA%8F%E5%88%97%EF%BC%8C%E6%A0%B9%E6%8D%AE%E5%B7%B2%E7%9F%A5%E7%9A%84%E6%84%9F%E7%9F%A5%E5%BA%8F%E5%88%97%E6%8F%90%E4%BE%9B%E7%9A%84%E8%AF%81%E6%8D%AE%E5%92%8C-Agent-%E5%85%B7%E6%9C%89%E7%9A%84%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86%EF%BC%8C%E7%90%86%E6%80%A7-Agent-%E5%BA%94%E8%AF%A5%E9%80%89%E6%8B%A9%E8%83%BD%E4%BD%BF%E5%85%B6%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E6%9C%80%E5%A4%A7%E5%8C%96%E7%9A%84%E8%A1%8C%E5%8A%A8%E3%80%82-2-2-2-%E5%85%A8%E7%9F%A5%E8%80%85-%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%87%AA%E4%B8%BB%E6%80%A7%E5%85%A8%E7%9F%A5%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%85%A8%E7%9F%A5%E7%9A%84-Agent-%E6%98%8E%E7%A1%AE%E5%9C%B0%E7%9F%A5%E9%81%93%E5%AE%83%E7%9A%84%E8%A1%8C%E5%8A%A8%E4%BA%A7%E7%94%9F%E7%9A%84%E5%AE%9E%E9%99%85%E7%BB%93%E6%9E%9C%E5%B9%B6%E4%B8%94%E5%81%9A%E5%87%BA%E7%9B%B8%E5%BA%94%E7%9A%84%E5%8A%A8%E4%BD%9C%E7%90%86%E6%80%A7%E6%98%AF%E4%BD%BF%E6%9C%9F%E6%9C%9B%E7%9A%84%E6%80%A7%E8%83%BD%E6%9C%80%E5%A4%A7%E5%8C%96%EF%BC%8C%E8%80%8C%E5%AE%8C%E7%BE%8E%E6%98%AF%E4%BD%BF%E5%AE%9E%E9%99%85%E7%9A%84%E6%80%A7%E8%83%BD%E6%9C%80%E5%A4%A7%E5%8C%96%E3%80%82%E5%AF%B9%E7%90%86%E6%80%A7%E7%9A%84%E5%AE%9A%E4%B9%89%E5%B9%B6%E4%B8%8D%E8%A6%81%E6%B1%82%E5%85%A8%E7%9F%A5%EF%BC%8C%E5%9B%A0%E4%B8%BA%E7%90%86%E6%80%A7%E7%9A%84%E9%80%89%E6%8B%A9%E5%8F%AA%E4%BE%9D%E8%B5%96%E4%BA%8E%E5%88%B0%E5%BD%93%E6%97%B6%E4%B8%BA%E6%AD%A2%E7%9A%84%E6%84%9F%E7%9F%A5%E5%BA%8F%E5%88%97%E3%80%82%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86%EF%BC%9A%E4%B8%BA%E4%BA%86%E4%BF%AE%E6%94%B9%E6%9C%AA%E6%9D%A5%E7%9A%84%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%E8%80%8C%E9%87%87%E5%8F%96%E8%A1%8C%E5%8A%A8%E3%80%82%E6%98%AF%E7%90%86%E6%80%A7%E7%9A%84%E9%87%8D%E8%A6%81%E9%83%A8%E5%88%86Agent-%E4%BE%9D%E8%B5%96%E4%BA%8E%E8%AE%BE%E8%AE%A1%E4%BA%BA%E5%91%98%E7%9A%84%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86%E8%80%8C%E4%B8%8D%E6%98%AF%E5%AE%83%E8%87%AA%E8%BA%AB%E7%9A%84%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%83%85%E5%86%B5%E6%88%91%E4%BB%AC%E4%BC%9A%E8%AF%B4%E8%AF%A5Agent-%E7%BC%BA%E4%B9%8F%E8%87%AA%E4%B8%BB%E6%80%A7%E3%80%82%E7%90%86%E6%80%A7%E7%9A%84Agent%E5%BA%94%E8%AF%A5%E6%98%AF%E8%87%AA%E4%B8%BB%E7%9A%84%EF%BC%9A%E5%BA%94%E8%AF%A5%E5%AD%A6%E4%B9%A0%E4%BB%A5%E5%BC%A5%E8%A1%A5%E4%B8%8D%E5%AE%8C%E6%95%B4%E7%9A%84%E6%88%96%E8%80%85%E4%B8%8D%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86%E3%80%82-2-3-%E7%8E%AF%E5%A2%83%E7%9A%84%E6%80%A7%E8%B4%A8%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%90%86%E6%80%A7Agent%EF%BC%8C%E9%A6%96%E5%85%88%E8%80%83%E8%99%91%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83-2-3-1-%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E7%9A%84%E8%A7%84%E8%8C%83%E6%8F%8F%E8%BF%B0Agent-%E7%9A%84%E7%90%86%E6%80%A7%EF%BC%8C%E6%88%91%E4%BB%AC%E5%BF%85%E9%A1%BB%E8%A7%84%E5%AE%9A%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E3%80%81%E7%8E%AF%E5%A2%83%E4%BB%A5%E5%8F%8A-Agent%E7%9A%84%E6%89%A7%E8%A1%8C%E5%99%A8%E5%92%8C%E4%BC%A0%E6%84%9F%E5%99%A8%E3%80%82%E6%8A%8A%E6%89%80%E6%9C%89%E8%BF%99%E4%BA%9B%E5%BD%92%E5%9C%A8%E4%B8%80%E8%B5%B7%EF%BC%8C%E9%83%BD%E5%B1%9E%E4%BA%8E%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E3%80%82%E6%A0%B9%E6%8D%AE%E9%A6%96%E5%AD%97%E6%AF%8D%E7%BC%A9%E5%86%99%EF%BC%8C%E6%88%91%E4%BB%AC%E7%A7%B0%E4%B9%8B%E4%B8%BA-PEAS-%E6%8F%8F%E8%BF%B0%EF%BC%88Performance-%E6%80%A7%E8%83%BD%EF%BC%89%EF%BC%8C-Environment-%E7%8E%AF%E5%A2%83%EF%BC%89%EF%BC%8C-Actuators-%E6%89%A7%E8%A1%8C%E5%99%A8%EF%BC%89%EF%BC%8C-Sensors-%E4%BC%A0%E6%84%9F%E5%99%A8%EF%BC%89%EF%BC%89%E3%80%82-2-3-2%E5%AF%B9%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%BB%B4%E5%BA%A6%E5%BE%88%E5%A4%A7%E7%A8%8B%E5%BA%A6%E4%B8%8A%E5%86%B3%E5%AE%9A%E4%BA%86Agent%E7%9A%84%E8%AE%BE%E8%AE%A1%E3%80%82%E5%AE%8C%E5%85%A8%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%9A%84%E4%B8%8E%E9%83%A8%E5%88%86%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%9A%84%EF%BC%9A-%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%9A%84%EF%BC%9A%E5%9C%A8%E6%AF%8F%E4%B8%AA%E6%97%B6%E9%97%B4%E7%82%B9%E4%B8%8A%E9%83%BD%E8%83%BD%E8%8E%B7%E5%8F%96%E7%8E%AF%E5%A2%83%E7%9A%84%E5%AE%8C%E6%95%B4%E7%8A%B6%E6%80%81%EF%BC%8C%E9%82%A3%E4%B9%88%E7%8E%AF%E5%A2%83%E5%B0%B1%E6%98%AF%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%9A%84%E3%80%82-%E5%A6%82%E6%9E%9C%E4%BC%A0%E6%84%9F%E5%99%A8%E8%83%BD%E5%A4%9F%E6%A3%80%E6%B5%8B%E6%89%80%E6%9C%89%E4%B8%8E%E8%A1%8C%E5%8A%A8%E5%86%B3%E7%AD%96%E4%B8%8B%E9%82%A3%E4%B8%AA%E5%85%B3%E5%BF%83%E7%9A%84%E4%BF%A1%E6%81%AF%EF%BC%8C%E9%82%A3%E4%B9%88%E8%AF%A5%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E6%98%AF%E5%AE%8C%E5%85%A8-%E6%9C%89%E6%95%88%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%9A%84%E3%80%82%E5%8D%95Agent%E4%B8%8E%E5%A4%9AAgent%EF%BC%9A-%E5%85%B3%E9%94%AE%E7%9A%84%E5%8C%BA%E5%88%AB%E5%9C%A8%E4%BA%8E-B-%E7%9A%84%E8%A1%8C%E4%B8%BA%E6%98%AF%E5%90%A6%E5%AF%BB%E6%B1%82%E8%AE%A9%E4%BE%9D%E8%B5%96%E4%BA%8E-Agent-A-%E7%9A%84%E8%A1%8C%E4%B8%BA%E7%9A%84%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E5%80%BC%E6%9C%80%E5%A4%A7%E5%8C%96%E7%A1%AE%E5%AE%9A%E7%9A%84%E4%B8%8E%E9%9A%8F%E6%9C%BA%E7%9A%84%EF%BC%9A-%E5%A6%82%E6%9E%9C%E7%8E%AF%E5%A2%83%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%8A%B6%E6%80%81%E5%AE%8C%E5%85%A8%E5%8F%96%E5%86%B3%E4%BA%8E%E5%BD%93%E5%89%8D%E7%8A%B6%E6%80%81%E5%92%8CAgent%E6%89%A7%E8%A1%8C%E7%9A%84%E5%8A%A8%E4%BD%9C%EF%BC%8C%E9%82%A3%E4%B9%88%E6%88%91%E4%BB%AC%E8%AF%B4%E8%AF%A5%E7%8E%AF%E5%A2%83%E6%98%AF%E7%A1%AE%E5%AE%9A%E7%9A%84%EF%BC%8C%E5%90%A6%E5%88%99%E4%BB%96%E6%98%AF%E9%9A%8F%E6%9C%BA%E7%9A%84%E3%80%82%E7%89%87%E6%AE%B5%E7%9A%84%E4%B8%8E%E5%BB%B6%E7%BB%AD%E5%BC%8F%E7%9A%84%EF%BC%9A-%E5%9C%A8%E7%89%87%E6%AE%B5%E5%BC%8F%E7%9A%84%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E4%B8%AD%EF%BC%8CAgent-%E7%9A%84%E7%BB%8F%E5%8E%86%E8%A2%AB%E5%88%86%E6%88%90%E4%BA%86%E4%B8%80%E4%B8%AA%E4%B8%AA%E5%8E%9F%E5%AD%90%E7%89%87%E6%AE%B5%E3%80%82%E5%9C%A8%E6%AF%8F%E4%B8%AA%E7%89%87%E6%AE%B5%E4%B8%AD-Agent-%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%E5%B9%B6%E5%AE%8C%E6%88%90%E5%8D%95%E4%B8%AA%E8%A1%8C%E5%8A%A8%E3%80%82%E5%85%B3%E9%94%AE%E7%9A%84%E6%98%AF%EF%BC%8C%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%89%87%E6%AE%B5%E4%B8%8D%E4%BE%9D%E8%B5%96%E4%BA%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E7%89%87%E6%AE%B5%E4%B8%AD%E9%87%87%E5%8F%96%E7%9A%84%E8%A1%8C%E5%8A%A8%E3%80%82%E5%BE%88%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E5%B1%9E%E4%BA%8E%E7%89%87%E6%AE%B5%E5%BC%8F%E7%9A%84%E9%9D%99%E6%80%81%E7%9A%84%E4%B8%8E%E5%8A%A8%E6%80%81%E7%9A%84%EF%BC%9A-%E5%A6%82%E6%9E%9C%E7%8E%AF%E5%A2%83%E5%9C%A8Agent-%E8%AE%A1%E7%AE%97%E7%9A%84%E6%97%B6%E5%80%99%E4%BC%9A%E5%8F%98%E5%8C%96%EF%BC%8C%E9%82%A3%E4%B9%88%E6%98%AF%E5%8A%A8%E6%80%81%E7%9A%84%E3%80%82%E7%A6%BB%E6%95%A3%E7%9A%84%E4%B8%8E%E8%BF%9E%E7%BB%AD%E7%9A%84%EF%BC%9A-%E7%8E%AF%E5%A2%83%E7%9A%84%E7%8A%B6%E6%80%81%E3%80%81%E6%97%B6%E9%97%B4%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F%E4%BB%A5%E5%8F%8AAgent%E7%9A%84%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%E5%92%8C%E8%A1%8C%E5%8A%A8%EF%BC%8C%E9%83%BD%E6%9C%89%E7%A6%BB%E6%95%A3-%E8%BF%9E%E7%BB%AD%E4%B9%8B%E5%88%86%E3%80%82%E5%B7%B2%E7%9F%A5%E7%9A%84%E5%92%8C%E6%9C%AA%E7%9F%A5%E7%9A%84%EF%BC%9A-%E8%BF%99%E7%A7%8D%E5%8C%BA%E5%88%86%E6%8C%87%E7%9A%84%E4%B8%8D%E6%98%AF%E7%8E%AF%E5%A2%83%E6%9C%AC%E8%BA%AB%EF%BC%8C%E6%8C%87%E7%9A%84%E6%98%AFAgent%E7%9A%84%E5%8F%AA%E6%98%AF%E7%8A%B6%E6%80%81%EF%BC%8C%E8%BF%99%E9%87%8C%E7%9A%84%E5%8F%AA%E6%98%AF%E5%88%99%E6%98%AF%E5%80%BC%E7%8E%AF%E5%A2%83%E7%9A%84%E2%80%9C%E7%89%A9%E7%90%86%E6%B3%95%E5%88%99%E2%80%9D%E3%80%82-2-4-Agent%E7%9A%84%E7%BB%93%E6%9E%84AI%E7%9A%84%E4%BB%BB%E5%8A%A1%E6%98%AF%E8%AE%BE%E8%AE%A1Agent%E7%A8%8B%E5%BA%8F%EF%BC%8C%E5%AE%83%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%98%AF%E6%8A%8A%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%E6%98%A0%E5%B0%84%E5%88%B0%E8%A1%8C%E5%8A%A8%E7%9A%84Agent%E5%87%BD%E6%95%B0%E3%80%82%E5%81%87%E8%AE%BE%E8%AF%A5%E7%A8%8B%E5%BA%8F%E8%A6%81%E5%9C%A8%E6%9F%90%E4%B8%AA%E5%85%B7%E5%A4%87%E6%97%A0%E5%8A%9B%E4%BC%A0%E6%84%9F%E5%99%A8%E5%92%8C%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%A1%E7%AE%97%E8%A3%85%E7%BD%AE%E4%B8%8A%E8%BF%90%E8%A1%8C%EF%BC%8C%E6%88%91%E4%BB%AC%E6%88%90%E4%B8%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E3%80%82Agent-%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-%E7%A8%8B%E5%BA%8F-2-4-1Agent%E7%A8%8B%E5%BA%8F-Agent-%E7%A8%8B%E5%BA%8F%E9%83%BD%E5%85%B7%E6%9C%89%E5%90%8C%E6%A0%B7%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%9A%E8%BE%93%E5%85%A5%E4%B8%BA%E4%BB%8E%E4%BC%A0%E6%84%9F%E5%99%A8%E5%BE%97%E5%88%B0%E7%9A%84%E5%BD%93%E5%89%8D%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%EF%BC%8C%E8%BF%94%E5%9B%9E%E7%9A%84%E6%98%AF%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%A1%8C%E5%8A%A8%E6%8A%89%E6%8B%A9-2-4-2-%E7%AE%80%E5%8D%95%E5%8F%8D%E5%B0%84Agent%E7%AE%80%E5%8D%95%E5%8F%8D%E7%9D%80Agent%EF%BC%9A-%E6%98%AF%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A7%8D%E7%B1%BB-%E5%9F%BA%E4%BA%8E%E5%BD%93%E5%89%8D%E7%9A%84%E6%84%9F%E7%9F%A5%E9%80%89%E6%8B%A9%E8%A1%8C%E5%8A%A8%EF%BC%8C%E4%B8%8D%E5%85%B3%E6%B3%A8%E6%84%9F%E7%9F%A5%E5%8E%86%E5%8F%B2%E3%80%82%E5%8D%B3%E7%AC%A6%E5%90%88%E6%9D%A1%E4%BB%B6%E9%82%A3%E4%B9%88%E5%B0%B1%E6%89%A7%E8%A1%8C%EF%BC%8C%E8%BF%99%E6%98%AF%E6%9D%A1%E4%BB%B6-%E8%A1%8C%E4%B8%BA%E8%A7%84%E5%88%99%E3%80%82%E5%A6%82%E6%9E%9C%E4%BB%80%E4%B9%88%E4%BB%80%E4%B9%88-%E9%82%A3%E4%B9%88%E6%80%8E%E4%B9%88%E6%80%8E%E4%B9%88%E7%BC%BA%E7%82%B9%EF%BC%9A%E6%99%BA%E8%83%BD%E6%9C%89%E9%99%90-%E4%BC%9A%E9%99%B7%E5%85%A5%E6%97%A0%E9%99%90%E7%9A%84%E5%BE%AA%E7%8E%AF%E8%A7%A3%E5%86%B3%EF%BC%9AAgent%E7%9A%84%E8%A1%8C%E5%8A%A8%E8%83%BD%E5%A4%9F%E9%9A%8F%E6%9C%BA%E5%8C%96-2-4-3-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%8D%E5%B0%84Agent%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86%E5%8F%AF%E8%A7%82%E6%B5%8B%E7%8E%AF%E5%A2%83%E7%9A%84%E6%9C%80%E6%9C%89%E6%95%88%E9%80%94%E5%BE%84%E6%98%AF%E8%AE%A9Agent%E8%B7%9F%E8%B8%AA%E8%AE%B0%E5%BD%95%E7%8E%B0%E5%9C%A8%E7%9C%8B%E4%B8%8D%E5%88%B0%E7%9A%84%E9%82%A3%E9%83%A8%E5%88%86%E4%B8%96%E7%95%8C%E3%80%82%E5%8D%B3Agent%E5%BA%94%E8%AF%A5%E6%A0%B9%E6%8D%AE%E6%84%9F%E7%9F%A5%E5%8E%86%E5%8F%B2%E7%BB%B4%E6%8C%81%E5%86%85%E9%83%A8%E7%8A%B6%E6%80%81%EF%BC%8C%E4%BB%8E%E8%80%8C%E8%87%B3%E5%B0%91%E5%8F%8D%E6%98%A0%E5%87%BA%E5%BD%93%E5%89%8D%E7%8A%B6%E6%80%81%E7%9C%8B%E4%B8%8D%E5%88%B0%E7%9A%84%E4%BF%A1%E6%81%AF%E3%80%82%E9%9C%80%E8%A6%81%E5%9C%A8%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%8A%A0%E5%85%A5%E4%B8%A4%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%9F%A5%E8%AF%86%EF%BC%9A%E9%A6%96%E5%85%88%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E4%B8%96%E7%95%8C%E6%98%AF%E5%A6%82%E4%BD%95%E7%8B%AC%E7%AB%8B%E4%BA%8EAgent%E8%80%8C%E5%8F%91%E5%B1%95%E7%9A%84%E4%BF%A1%E6%81%AF-%E5%85%B6%E6%AC%A1%EF%BC%8C%E7%9F%A5%E9%81%93Aagent%E8%87%AA%E8%BA%AB%E7%9A%84%E8%A1%8C%E5%8A%A8%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E4%B8%96%E7%95%8C%E7%9A%84%E4%BF%A1%E6%81%AF%E3%80%82%E8%BF%99%E7%A7%8D%E6%A8%A1%E5%9E%8B%E7%9A%84Agent%E8%A2%AB%E7%A7%B0%E4%B8%BA%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84Agent%E5%85%B3%E4%BA%8E-Agent-%E7%9B%AE%E7%9A%84%E5%9C%B0%E7%9A%84%E4%BA%8B%E5%AE%9E%E6%89%8D%E6%98%AF%E7%9C%9F%E6%AD%A3%E7%9A%84-Agent-%E5%86%85%E9%83%A8%E7%8A%B6%E6%80%81%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B9%E9%9D%A2-2-4-4-%E5%9F%BA%E4%BA%8E%E7%9B%AE%E6%A0%87%E7%9A%84AgentAgent%E8%BF%98%E9%9C%80%E8%A6%81%E7%9B%AE%E6%A0%87%E4%BF%A1%E6%81%AF%E6%9D%A5%E6%8F%8F%E8%BF%B0%E6%83%B3%E8%A6%81%E8%BE%BE%E5%88%B0%E7%9A%84%E7%8A%B6%E5%86%B5%E6%90%9C%E7%B4%A2%E5%92%8C%E8%A7%84%E5%88%92%E6%98%AF%E5%AF%BB%E6%89%BE%E8%BE%BE%E6%88%90Agent%E7%9B%AE%E6%A0%87%E7%9A%84%E8%A1%8C%E5%8A%A8%E5%BA%8F%E5%88%97%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9D%A1%E4%BB%B6-%E8%A1%8C%E5%8A%A8%E4%B8%8D%E5%90%8C%EF%BC%8C%E5%9B%A0%E4%B8%BA%E4%BB%96%E8%80%83%E8%99%91%E4%BA%86%E6%9C%AA%E6%9D%A5%E5%B0%BD%E7%AE%A1%E6%95%88%E7%8E%87%E4%BD%8E%EF%BC%8C%E4%BD%86%E6%98%AF%E6%9B%B4%E5%8A%A0%E7%81%B5%E6%B4%BB-2-4-5-%E5%9F%BA%E4%BA%8E%E6%95%88%E7%94%A8%E7%9A%84Agent%E4%BB%85%E9%9D%A0%E7%9B%AE%E6%A0%87%E4%B8%8D%E8%B6%B3%E4%BB%A5%E7%94%9F%E6%88%90%E9%AB%98%E5%93%81%E8%B4%A8%E7%9A%84%E8%A1%8C%E4%B8%BA%E3%80%82%E6%95%88%E7%94%A8%EF%BC%9AAgent%E7%9A%84%E6%95%88%E7%94%A8%E5%87%BD%E6%95%B0%E6%98%AF%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E7%9A%84%E5%86%85%E5%9C%A8%E5%8C%96%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%86%85%E5%9C%A8%E7%9A%84%E6%95%88%E7%94%A8%E5%87%BD%E6%95%B0%E5%92%8C%E5%A4%96%E5%9C%A8%E7%9A%84%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E6%98%AF%E5%92%8C%E8%B0%90%E7%9A%84%EF%BC%8C%E9%82%A3%E4%B9%88%E9%80%89%E6%8B%A9%E6%9C%80%E5%A4%A7%E6%95%88%E7%94%A8%E8%A1%8C%E5%8A%A8%E7%9A%84Agent%E6%A0%B9%E6%8D%AE%E5%A4%96%E5%9C%A8%E7%9A%84%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E4%B9%9F%E6%98%AF%E7%90%86%E6%80%A7%E7%9A%84%E7%AC%AC%E4%B8%80%EF%BC%8C%E5%BD%93%E5%A4%9A%E4%B8%AA%E7%9B%AE%E6%A0%87%E4%BA%92%E7%9B%B8%E5%86%B2%E7%AA%81%E6%97%B6%EF%BC%8C%E5%8F%AA%E6%9C%89%E5%85%B6%E4%B8%AD%E4%B8%80%E4%BA%9B%E7%9B%AE%E6%A0%87%E5%8F%AF%E4%BB%A5%E8%BE%BE%E5%88%B0%E6%97%B6%EF%BC%88%E4%BE%8B%E5%A6%82%EF%BC%8C%E9%80%9F%E5%BA%A6%E5%92%8C%E5%AE%89%E5%85%A8%E6%80%A7-%EF%BC%8C%E6%95%88%E7%94%A8%E5%87%BD%E6%95%B0%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E9%80%82%E5%BD%93%E7%9A%84%E6%8A%98%E4%B8%AD%E3%80%82%E7%AC%AC%E4%BA%8C%EF%BC%8C%E5%BD%93-Agent-%E6%9C%89%E5%87%A0%E4%B8%AA%E7%9B%AE%E6%A0%87%EF%BC%8C%E4%BD%86%E6%B2%A1%E6%9C%89%E4%B8%80%E4%B8%AA%E6%9C%89%E6%8A%8A%E6%8F%A1%E8%BE%BE%E5%88%B0%E6%97%B6%EF%BC%8C%E6%95%88%E7%94%A8%E5%87%BD%E6%95%B0%E5%8F%AF%E4%BB%A5%E6%A0%B9%E6%8D%AE%E7%9B%AE%E6%A0%87%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E5%AF%B9%E6%88%90%E5%8A%9F%E7%9A%84%E4%BC%BC%E7%84%B6%E7%8E%87%E5%8A%A0%E6%9D%83%E7%90%86%E6%80%A7%E7%9A%84%E5%9F%BA%E4%BA%8E%E6%95%88%E7%94%A8%E7%9A%84-Agent-%E9%80%89%E6%8B%A9%E4%BD%BF%E5%85%B6%E6%9C%9F%E6%9C%9B%E6%95%88%E7%94%A8%E6%9C%80%E5%A4%A7%E5%8C%96%E7%9A%84%E8%A1%8C%E5%8A%A8-2-4-6-%E5%AD%A6%E4%B9%A0Agent%E5%AD%A6%E4%B9%A0-Agent-%E5%8F%AF%E4%BB%A5%E8%A2%AB%E5%88%92%E5%88%86%E4%B8%BA-4-%E4%B8%AA%E6%A6%82%E5%BF%B5%E4%B8%8A%E7%9A%84%E7%BB%84%E4%BB%B6%EF%BC%9ACritic-Learning-element-problem-generator-Performance-element%EF%BC%8C%E9%87%8D%E7%82%B9%E4%BD%93%E7%8E%B0%E5%9C%A8%E5%AD%A6%E4%B9%A0%E5%85%83%E4%BB%B6%E5%92%8C%E6%80%A7%E8%83%BD%E5%85%83%E4%BB%B6%E3%80%82%E5%AD%A6%E4%B9%A0%E5%85%83%E4%BB%B6%E8%B4%9F%E8%B4%A3%E6%94%B9%E8%BF%9B%E6%8F%90%E9%AB%98%EF%BC%8C%E8%80%8C%E6%80%A7%E8%83%BD%E5%85%83%E4%BB%B6%E8%B4%9F%E8%B4%A3%E9%80%89%E6%8B%A9%E5%A4%96%E9%83%A8%E8%A1%8C%E5%8A%A8%E3%80%82%E6%80%A7%E8%83%BD%E5%85%83%E4%BB%B6%E6%98%AF%E6%88%91%E4%BB%AC%E5%89%8D%E9%9D%A2%E8%80%83%E8%99%91%E7%9A%84%E6%95%B4%E4%B8%AA-Agent-%E5%AE%83%E6%8E%A5%E5%8F%97%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%E5%B9%B6%E5%86%B3%E7%AD%96%E3%80%82%E5%AD%A6%E4%B9%A0%E5%85%83%E4%BB%B6%E5%88%A9%E7%94%A8%E6%9D%A5%E8%87%AA%E8%AF%84%E5%88%A4%E5%85%83%E4%BB%B6%E7%9A%84%E5%8F%8D%E9%A6%88%E8%AF%84%E4%BB%B7-Agent-%E5%81%9A%E5%BE%97%E5%A6%82%E4%BD%95%EF%BC%8C%E5%B9%B6%E7%A1%AE%E5%AE%9A%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%E6%80%A7%E8%83%BD%E5%85%83%E4%BB%B6%E4%BB%A5%E4%BE%BF%E5%B0%86%E6%9D%A5%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%E5%AD%A6%E4%B9%A0%E5%85%83%E4%BB%B6%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%BE%88%E5%A4%A7%E7%A8%8B%E5%BA%A6%E4%B8%8A%E4%BE%9D%E8%B5%96%E4%BA%8E%E6%80%A7%E8%83%BD%E5%85%83%E4%BB%B6%E7%9A%84%E8%AE%BE%E8%AE%A1%E8%AF%84%E5%88%A4%E5%85%83%E4%BB%B6%E6%A0%B9%E6%8D%AE%E5%9B%BA%E5%AE%9A%E7%9A%84%E6%80%A7%E8%83%BD%E6%A0%87%E5%87%86%E5%91%8A%E8%AF%89%E5%AD%A6%E4%B9%A0%E5%85%83%E4%BB%B6-Agent-%E7%9A%84%E8%BF%90%E8%BD%AC%E6%83%85%E5%86%B5%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E7%BB%84%E4%BB%B6%E6%98%AF%E9%97%AE%E9%A2%98%E4%BA%A7%E7%94%9F%E5%99%A8%E3%80%82%E5%AE%83%E8%B4%9F%E8%B4%A3%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E6%96%B0%E7%9A%84%E5%92%8C%E6%9C%89%E4%BF%A1%E6%81%AF%E7%9A%84%E7%BB%8F%E9%AA%8C%E7%9A%84%E8%A1%8C%E5%8A%A8%E6%8F%90%E8%AE%AE-2-4-7-Agent%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%90%84%E7%BB%84%E4%BB%B6%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E6%88%91%E4%BB%AC%E5%B0%86%E8%A1%A8%E7%A4%BA%E6%94%BE%E7%BD%AE%E5%9C%A8%E4%B8%8D%E6%96%AD%E5%A2%9E%E9%95%BF%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%92%8C%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BD%B4%E7%BA%BF%E4%B8%8A%EF%BC%9A%E5%8E%9F%E5%AD%90%E3%80%81%E8%A6%81%E7%B4%A0%E5%92%8C%E7%BB%93%E6%9E%84-%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E9%80%9A%E8%BF%87%E6%90%9C%E7%B4%A2%E8%BF%9B%E8%A1%8C%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3-Agent-%E4%BD%BF%E7%94%A8%E5%8E%9F%E5%AD%90%EF%BC%88atomic-%E8%A1%A8%E7%A4%BA%EF%BC%9A%E4%B8%96%E7%95%8C%E7%9A%84%E7%8A%B6%E6%80%81%E8%A2%AB%E8%A7%86%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%EF%BC%8C%E5%AF%B9%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95%E8%80%8C%E8%A8%80%E6%B2%A1%E6%9C%89%E5%8F%AF%E8%A7%81%E7%9A%84%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84%E3%80%82%E4%BD%BF%E7%94%A8%E6%9B%B4%E5%85%88%E8%BF%9B%E7%9A%84%E8%A6%81%E7%B4%A0%E5%8C%96%E6%88%96%E7%BB%93%E6%9E%84%E5%8C%96%E8%A1%A8%E7%A4%BA%E7%9A%84%E5%9F%BA%E4%BA%8E%E7%9B%AE%E6%A0%87%E7%9A%84-Agent-%E9%80%9A%E5%B8%B8%E8%A2%AB%E7%A7%B0%E4%B8%BA%E8%A7%84%E5%88%92-Agent%E6%97%A0%E4%BF%A1%E6%81%AF%E7%9A%84%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%EF%BC%9A%E9%99%A4%E4%BA%86%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89%E6%9C%AC%E8%BA%AB%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E5%85%B6%E4%BB%96%E4%BF%A1%E6%81%AF%E6%9C%89%E4%BF%A1%E6%81%AF%E7%9A%84%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%EF%BC%9A%E5%8F%AF%E4%BB%A5%E5%88%A9%E7%94%A8%E7%BB%99%E5%AE%9A%E7%9A%84%E5%80%BC%E6%98%AF%E5%BC%95%E5%AF%BC%E8%83%BD%E5%A4%9F%E6%9C%89%E6%95%88%E7%9A%84%E6%89%BE%E5%88%B0%E8%A7%A3-3-1-%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3agent%E4%B8%89%E6%AD%A5%E8%B5%B0%EF%BC%9A%E5%BD%A2%E5%BC%8F%E5%8C%96%E3%80%81%E6%90%9C%E7%B4%A2%E3%80%81%E6%89%A7%E8%A1%8C%E5%BD%A2%E5%BC%8F%E5%8C%96%EF%BC%9A%E5%B8%AE%E5%8A%A9-Agent-%E7%BB%84%E7%BB%87%E8%A1%8C%E5%8A%A8%E5%BA%8F%E5%88%97%EF%BC%8C%E4%BB%A5%E8%BE%BE%E5%88%B0%E6%9C%80%E7%BB%88%E7%9B%AE%E6%A0%87%E3%80%82%E5%9F%BA%E4%BA%8E%E5%BD%93%E5%89%8D%E7%9A%84%E6%83%85%E5%BD%A2%E5%92%8C-Agent-%E7%9A%84%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E5%BD%A2%E5%BC%8F%E5%8C%96%EF%BC%88goal-formulation-%E6%98%AF%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%E3%80%82%E9%97%AE%E9%A2%98%E5%BD%A2%E5%BC%8F%E5%8C%96%EF%BC%88problem-formulation-%E6%98%AF%E5%9C%A8%E7%BB%99%E5%AE%9A%E7%9B%AE%E6%A0%87%E4%B8%8B%E7%A1%AE%E5%AE%9A%E9%9C%80%E8%A6%81%E8%80%83%E8%99%91%E5%93%AA%E4%BA%9B%E8%A1%8C%E5%8A%A8%E5%92%8C%E7%8A%B6%E6%80%81%E7%9A%84%E8%BF%87%E7%A8%8B%E6%90%9C%E7%B4%A2%EF%BC%9A%E4%B8%BA%E8%BE%BE%E5%88%B0%E7%9B%AE%E6%A0%87%EF%BC%8C%E5%AF%BB%E6%89%BE%E8%BF%99%E6%A0%B7%E7%9A%84%E8%A1%8C%E5%8A%A8%E5%BA%8F%E5%88%97%E7%9A%84%E8%BF%87%E7%A8%8B%E8%A2%AB%E7%A7%B0%E4%B8%BA%E6%90%9C%E7%B4%A2%EF%BC%9A%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E9%97%AE%E9%A2%98%EF%BC%8C%E8%BE%93%E5%87%BA%E6%98%AF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%EF%BC%8C%E4%BB%A5%E8%A1%8C%E5%8A%A8%E5%BA%8F%E5%88%97%E7%9A%84%E5%BD%A2%E5%BC%8F%E8%BF%94%E5%9B%9E%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E3%80%82%E6%89%A7%E8%A1%8C%EF%BC%9A%E6%89%A7%E8%A1%8C%E5%BB%BA%E8%AE%AE%E7%9A%84%E8%A1%8C%E5%8A%A8%E5%BC%80%E7%8E%AF%E7%B3%BB%E7%BB%9F%EF%BC%9A%E5%8D%81%E5%88%86%E7%A1%AE%E5%AE%9A%E8%A1%8C%E5%8A%A8%E7%9A%84%E5%90%8E%E6%9E%9C%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E5%AE%83%E6%97%A0%E8%A7%86%E5%AE%83%E7%9A%84%E6%84%9F%E7%9F%A5%E4%BF%A1%E6%81%AF%E3%80%82-3-1-1-%E8%89%AF%E5%AE%9A%E4%B9%89%E7%9A%84%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E8%A7%A3%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E5%8F%AF%E4%BB%A5%E7%94%A85%E4%B8%AA%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%E5%BD%A2%E5%BC%8F%E5%8C%96%E5%9C%B0%E6%8F%8F%E8%BF%B0%EF%BC%9A1-Agent%E7%9A%84%E5%88%9D%E5%A7%8B%E7%8A%B6%E6%80%812-%E6%8F%8F%E8%BF%B0Agent%E7%9A%84%E5%8F%AF%E8%83%BD%E8%A1%8C%E5%8A%A8%EF%BC%8C%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E7%89%B9%E6%AE%8A%E7%8A%B6%E6%80%81s%EF%BC%8CAction%EF%BC%88s%EF%BC%89%E8%BF%94%E5%9B%9E%E5%9C%A8%E7%8A%B6%E6%80%81s%E4%B8%8B%E5%8F%AF%E4%BB%A5%E6%89%A7%E8%A1%8C%E7%9A%84%E5%8A%A8%E4%BD%9C%E9%9B%86%E5%90%88%E3%80%823-%E5%AF%B9%E6%AF%8F%E4%B8%AA%E8%A1%8C%E5%8A%A8%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%9A%E5%8D%B3%E8%BD%AC%E7%A7%BB%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%99%E4%B8%AA%E7%8A%B6%E6%80%81%E4%B8%8B%E5%9C%A8%E8%BF%99%E6%A0%B7%E7%9A%84%E6%93%8D%E4%BD%9C%E5%8F%AF%E4%BB%A5%E5%8E%BB%E5%88%B0%E5%93%AA%E9%87%8C%E3%80%824-%E7%9B%AE%E6%A0%87%E6%B5%8B%E8%AF%95-%E7%A1%AE%E5%AE%9A%E7%BB%99%E5%AE%9A%E7%9A%84%E7%8A%B6%E6%80%81%E6%98%AF%E4%B8%8D%E6%98%AF%E7%9B%AE%E6%A0%87%E7%8A%B6%E6%80%81%E3%80%82%E6%9C%89%E6%97%B6%E5%80%99%E7%9B%AE%E6%A0%87%E7%8A%B6%E6%80%81%E6%98%AF%E4%B8%80%E4%B8%AA%E6%98%BE%E5%BC%8F%E9%9B%86%E5%90%88%EF%BC%8C%E6%B5%8B%E8%AF%95%E5%8F%AA%E9%9C%80%E7%AE%80%E5%8D%95%E6%A3%80%E6%9F%A5%E7%BB%99%E5%AE%9A%E7%9A%84%E7%8A%B6%E6%80%81%E6%98%AF%E5%90%A6%E5%9C%A8%E7%9B%AE%E6%A0%87%E7%8A%B6%E6%80%81%E9%9B%86%E5%90%88%E4%B8%AD5-%E8%B7%AF%E5%BE%84%E8%80%97%E6%95%A3%EF%BC%9A%E8%B7%AF%E5%BE%84%E8%80%97%E6%95%A3%E5%87%BD%E6%95%B0%E4%B8%BA%E6%AF%8F%E6%9D%A1%E8%B7%AF%E5%BE%84%E8%B5%8B%E4%B8%80%E4%B8%AA%E8%80%97%E6%95%A3%E5%80%BC%EF%BC%8C%E5%8D%B3%E7%94%B2%E9%86%9B%E7%99%BE%E5%B9%B4%E3%80%82%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3Agent%E9%80%89%E6%8B%A9%E8%83%BD%E5%8F%8D%E6%98%A0%E5%AE%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E7%9A%84%E8%80%97%E6%95%A3%E5%87%BD%E6%95%B0%E3%80%82-3-1-2-%E9%97%AE%E9%A2%98%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96%E4%B8%80%E8%88%AC%E6%83%85%E5%86%B5%E4%B8%8B%E9%83%BD%E8%A6%81%E6%8A%BD%E8%B1%A1%EF%BC%8C%E6%8A%BD%E8%B1%A1%E6%88%90%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E3%80%82%E8%A6%81%E5%AF%B9%E7%8A%B6%E6%80%81%E6%8F%8F%E8%BF%B0%E6%8A%BD%E8%B1%A1%EF%BC%8C%E8%A6%81%E5%AF%B9%E8%A1%8C%E5%8A%A8%E8%BF%9B%E8%A1%8C%E6%8A%BD%E8%B1%A1%E3%80%82%E5%A6%82%E4%BD%95%E7%B2%BE%E7%A1%AE%E7%9A%84%E5%AE%9A%E4%B9%89%E5%90%88%E9%80%82%E7%9A%84%E6%8A%BD%E8%B1%A1%E5%B1%82%E6%AC%A1%EF%BC%9A%E9%80%89%E6%8B%A9%E6%8A%BD%E8%B1%A1%E5%8C%96%E7%9A%84%E7%8A%B6%E6%80%81%E5%92%8C%E8%A1%8C%E5%8A%A8%E3%80%82%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E8%83%BD%E5%A4%9F%E6%8A%8A%E4%BB%BB%E4%BD%95%E6%8A%BD%E8%B1%A1%E8%A7%A3%E6%89%A9%E5%B1%95%E6%88%90%E4%B8%BA%E6%9B%B4%E7%BB%86%E8%8A%82%E7%9A%84%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E8%A7%A3%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%8A%BD%E8%B1%A1%E5%B0%B1%E6%98%AF%E6%9C%89%E6%95%88%E7%9A%84-3-2-%E9%97%AE%E9%A2%98%E5%AE%9E%E4%BE%8B%E5%88%86%E5%88%AB%E8%AE%A8%E8%AE%BA%E7%8E%A9%E5%85%B7%E9%97%AE%E9%A2%98%E5%92%8C%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%E9%97%AE%E9%A2%98%E3%80%82-3-2-1-%E7%8E%A9%E5%85%B7%E9%97%AE%E9%A2%98%E8%AE%A8%E8%AE%BA%E4%BA%86%E5%90%B8%E5%B0%98%E5%99%A8%E7%9A%84%E5%85%B7%E4%BD%93%E5%BA%94%E7%94%A8%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%AB%E6%95%B0%E7%A0%81%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E5%85%AB%E7%9A%87%E5%90%8E%E9%97%AE%E9%A2%98%EF%BC%88%E5%85%B7%E4%BD%93%E5%AE%9E%E4%BE%8B%EF%BC%89%E5%AF%B9%E4%BA%8E%E5%85%AB%E7%9A%87%E5%90%8E%E8%BF%99%E7%B1%BB%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A%E8%BF%99%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96%E4%B8%BB%E8%A6%81%E5%88%86%E4%B8%BA%E4%B8%A4%E7%B1%BB%EF%BC%9A%E5%A2%9E%E9%87%8F%E5%BD%A2%E5%BC%8F%E5%8C%96%EF%BC%88%E5%9C%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%98%AF%E4%BB%8E%E7%A9%BA%E7%8A%B6%E6%80%81%E5%BC%80%E5%A7%8B%E7%9A%84%EF%BC%89%E3%80%81%E5%AE%8C%E6%95%B4%E7%8A%B6%E6%80%81%E5%BD%A2%E5%BC%8F%E5%8C%96%E4%B8%8D%E8%AE%BA%E8%BF%99%E4%B8%A4%E7%A7%8D%E5%93%AA%E7%A7%8D%EF%BC%8C%E9%83%BD%E6%97%A0%E9%9C%80%E8%80%83%E8%99%91%E8%B7%AF%E5%BE%84%E6%B6%88%E8%80%97%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E8%80%83%E8%99%91%E6%9C%80%E7%BB%88%E7%8A%B6%E6%80%81%E3%80%82%E5%A2%9E%E9%87%8F%E5%BD%A2%E5%BC%8F%E8%AF%9D%E5%8F%AF%E4%BB%A5%E5%A6%82%E4%B8%8B%E8%80%83%E8%99%91%EF%BC%9A%E7%8A%B6%E6%80%81%EF%BC%9A%E5%88%9D%E5%A7%8B%E7%8A%B6%E6%80%81%EF%BC%9A%E8%A1%8C%E5%8A%A8%EF%BC%9A%E8%BD%AC%E7%A7%BB%E6%A8%A1%E5%9E%8B%EF%BC%9A%E7%9B%AE%E6%A0%87%E6%B5%8B%E8%AF%95%EF%BC%9A%E7%8A%B6%E6%80%81%EF%BC%9A%E8%A1%8C%E5%8A%A8%EF%BC%9A%E8%BF%98%E6%9C%89%E7%8E%A9%E5%85%B7%E9%97%AE%E9%A2%98%EF%BC%9A%E7%94%B1Donald-Knuth%E6%8F%90%E5%87%BA%EF%BC%8C%E5%8F%AA%E7%94%A8%E6%95%B0%E5%AD%974%EF%BC%8C%E4%B8%80%E4%B8%AA%E7%94%B1%E9%98%B6%E4%B9%98%E3%80%81%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%92%8C%E5%8F%96%E6%95%B4%E6%9E%84%E6%88%90%E7%9A%84%E6%93%8D%E4%BD%9C%E5%BA%8F%E5%88%97%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E4%BB%BB%E6%84%8F%E7%9A%84%E6%AD%A3%E6%95%B4%E6%95%B0%E3%80%82-3-2-2-%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%E9%97%AE%E9%A2%98%E5%AF%BB%E5%BE%84%E9%97%AE%E9%A2%98%E6%97%85%E8%A1%8C%E9%97%AE%E9%A2%98%E6%97%85%E8%A1%8C%E5%95%86%E9%97%AE%E9%A2%98VLSI%E5%B8%83%E7%BA%BF%E9%97%AE%E9%A2%98%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AF%BC%E8%88%AA%E9%97%AE%E9%A2%98-3-3-%E9%80%9A%E8%BF%87%E6%90%9C%E7%B4%A2%E6%B1%82%E8%A7%A3%E8%BF%9B%E8%A1%8C%E5%BD%A2%E5%BC%8F%E5%8C%96%E5%90%8E%EF%BC%8C%E5%AF%B9%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E6%B1%82%E8%A7%A3%EF%BC%9A%E4%B8%80%E4%B8%AA%E8%A7%A3%E6%98%AF%E4%B8%80%E4%B8%AA%E8%A1%8C%E5%8A%A8%E5%BA%8F%E5%88%97%EF%BC%8C%E6%89%80%E4%BB%A5%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%B0%B1%E6%98%AF%E8%80%83%E8%99%91%E5%90%84%E7%A7%8D%E5%8F%AF%E8%83%BD%E7%9A%84%E8%A1%8C%E5%8A%A8%E6%95%B0%E5%88%97%E3%80%82%E6%90%9C%E7%B4%A2%E6%A0%91%EF%BC%9A%E4%BB%8E%E6%A0%B9%E8%8A%82%E7%82%B9%E5%BC%80%E5%A7%8B%EF%BC%8C%E8%BF%9E%E7%BA%BF%E8%A1%A8%E7%A4%BA%E8%A1%8C%E5%8A%A8%EF%BC%8C%E8%8A%82%E7%82%B9%E5%AF%B9%E5%BA%94%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E7%8A%B6%E6%80%81%E3%80%82%E9%97%AE%E9%A2%98%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%B0%86%E8%A6%81%E6%89%A9%E5%B1%95%E7%9A%84%E7%8A%B6%E6%80%81L-%E5%8D%B3%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5-%E4%BB%A5%E9%81%BF%E5%85%8D%E5%86%97%E4%BD%99%E7%8A%B6%E6%80%81%E3%80%82"><span class="nav-text">一、人工智能## 1.1 什么是人工智能总述：像人一样思考 像人一样行动 合理的思考 合理地行动### 1.1.1 像人一样行动：图灵测试的途径图灵测试的定义：为智能提供一个令人满意的可操作的定义计算机需要具有的能力：自然语言处理、知识表达、自动推理、机器学习完全图灵测试：包括视频信号以便讯问者即可测试对方的感知能力，又有机会通过舱口船体物理对象计算机在以上四种还需要：计算机视觉、机器人学习以通过图灵测而研究AI不是一定的，例如航空工程不会吧目标定义为制造为能完全像鸽子一样飞行的机器，以至他们可以骗过其他鸽子 ### 1.1.2 像人一样思考：认知建模的途径前提确定人是如何思考的： 通过内省（捕获我们的思维过程）                                          通过心理实验（观察工作中的一个人）                                         通过脑成像（观察工作中的头脑）认知科学（cognitive science) 这个交叉学科领域把来自 AI的计算机模型与来自心理学的实验技术相结合试图构建一种精确且可测试的人类思维理论### 1.1.3 合理的思考：思维法则的途径三段论（亚里士多德）：在给定正确前提时总产生正确结论的论证结构提供了模式。 开创了逻辑学因此产生人工智能中的逻辑主义：希望能用逻辑来解决问题，给世上各个对象之间陈述制定一种精确的表示法### 1.1.4 合理地行动：合理agent的途径Agent：能够行动的某种东西Rational Agent：合理Agent是一个为了实现最佳结果，或者当存在不确定性时，为了实现最佳期望结果而行动的agent。合理agent的途径与其他途径相比有两个优点：因为正确的推理只是实现合理性的几种可能的机制之一且更经得起检验。## 1.2 人工智能的基础### 1.2.1 哲学大致历史的亚里士多德：阐述支配头脑理性部分的一组精确规则的人。Ramon Lull：有用的推理可以用机械人造物实现Thomas Hobbes：提出推理就像数值计算达芬奇：设计了一台机械计算器……笛卡尔是二元论的支持者：他认为人类头脑存在一部分在自然之外不受物理定律支配的东西 动物无之后对二元论的替换物是唯物主义（materialism）经验主义始于培根 然后再大卫休谟提出了归纳原理 ：一般规则通过揭示规则中元素之间的重复关联来获得逻辑实证主义：所有知识都可用最终与对应于感知输入的观察语句相联系的逻辑理论来刻画。头脑的哲学描述中的最后元素是知识与行动之间的联系。这个问题对人工智能是极其重要的，因为智能既要求推理又要求行动。亚里士多德提出：通过目标与行动结果的知识之间的逻辑关系来证明行动是正当的。后人利用这种算法成立了线性回归### 1.2.2 数学三个领域：逻辑 计算和概率逻辑上：布尔逻辑 弗雷格扩展了布尔逻辑这便是现在使用的一阶逻辑。第一个不平凡算法：最大公约数 欧几里得哥德尔的不完备性定理：一样强的任何形式理论中都存在不可判断的真语句但是有很多问题是不可及算的易处理性的概念具有更大的影响：相比于可判定性和可计算性不易处理：如果解决一个问题的实例所需时间随实例的规模成指数级地增长，那么该问题称为不易处理的如何确定不易处理的问题：斯蒂文库克和卡普：NP-completeness 证明了存在大量经典组合搜索与推理问题是 NP-完全的。NP-完全问题类可归约到的任何问题类可能就是不易处理的（一般认为np完全是不易处理的）概率：按照赌博事件的可能结果来描述它贝叶斯的规则构成了人工智能系统中大多数用于不确定推理的现代方法的基础### 1.2.3 经济学决策理论： 把概率理论和效用理论结合起来，为在不确定情况下，做出决策提供了一个形式化且完整的框架。### 1.2.4 神经科学### 1.2.5 心理学### 1.2.6 计算机工程# 二 智能agent## 2.1 agent 和环境agent 接受键盘敲击、文件内容和网络数据包作为传感器输入Agent 的感知序列是该 Agent 所收到的所有输入数据的完整历史。Agent 函数描述了 Agent 的行为，它将任意给定感知序列映射为行动Agent 程序则是具体实现，它在一些物理系统内部运行。就是函数是表达，程序是执行## 2.2 好的行为：理性的概念理性Agent是做事正确的Agent。渴望利用性能度量表述，它对环境状态的任何给定序列进行评估。作为一般原则，最好根据实际在环境中希望得到的结果来设计性能度量，而不是根据 Agent 表现出的行为。### 2.2.1 理性理性判断的四方面：定义成功标准的性能度量  Agent对环境的先验知识  Agent可以完成的行动  Agent截止到此时的感知序列从来得到理性的定义：对每一个可能的感知序列，根据已知的感知序列提供的证据和 Agent 具有的先验知识，理性 Agent 应该选择能使其性能度量最大化的行动。### 2.2.2 全知者 学习和自主性全知：一个全知的 Agent 明确地知道它的行动产生的实际结果并且做出相应的动作理性是使期望的性能最大化，而完美是使实际的性能最大化。对理性的定义并不要求全知，因为理性的选择只依赖于到当时为止的感知序列。信息收集：为了修改未来的感知信息而采取行动。是理性的重要部分Agent 依赖于设计人员的先验知识而不是它自身的感知信息，这种情况我们会说该Agent 缺乏自主性。理性的Agent应该是自主的：应该学习以弥补不完整的或者不正确的先验知识。## 2.3 环境的性质如何构建理性Agent，首先考虑任务环境### 2.3.1 任务环境的规范描述Agent 的理性，我们必须规定性能度量、环境以及 Agent的执行器和传感器。把所有这些归在一起，都属于任务环境。根据首字母缩写，我们称之为 PEAS 描述（Performance (性能）， Environment (环境）， Actuators (执行器）， Sensors(传感器））。### 2.3.2对任务环境进行分类，这些维度很大程度上决定了Agent的设计。完全可观察的与部分可观察的：    可观察的：在每个时间点上都能获取环境的完整状态，那么环境就是可观察的。                如果传感器能够检测所有与行动决策下那个关心的信息，那么该任务环境是完全                   有效可观察的。单Agent与多Agent：    关键的区别在于 B 的行为是否寻求让依赖于 Agent A 的行为的性能度量值最大化确定的与随机的：    如果环境的下一个状态完全取决于当前状态和Agent执行的动作，那么我们说该环境是确定的，否则他是随机的。片段的与延续式的：    在片段式的任务环境中，Agent 的经历被分成了一个个原子片段。在每个片段中 Agent 感知信息并完成单个行动。关键的是，下一个片段不依赖于以前的片段中采取的行动。很多分类任务属于片段式的静态的与动态的：    如果环境在Agent 计算的时候会变化，那么是动态的。离散的与连续的：    环境的状态、时间的处理方式以及Agent的感知信息和行动，都有离散&#x2F;连续之分。已知的和未知的：    这种区分指的不是环境本身，指的是Agent的只是状态，这里的只是则是值环境的“物理法则”。    ## 2.4 Agent的结构AI的任务是设计Agent程序，它实现的是把感知信息映射到行动的Agent函数。假设该程序要在某个具备无力传感器和执行器的计算装置上运行，我们成为体系结构。Agent &#x3D; 体系结构+程序### 2.4.1Agent程序 Agent 程序都具有同样的框架：输入为从传感器得到的当前感知信息，返回的是执行器的行动抉择### 2.4.2 简单反射Agent简单反着Agent： 是最简单的种类 基于当前的感知选择行动，不关注感知历史。即符合条件那么就执行，这是条件-行为规则。如果什么什么 那么怎么怎么缺点：智能有限 会陷入无限的循环解决：Agent的行动能够随机化 ### 2.4.3 基于模型的反射Agent处理部分可观测环境的最有效途径是让Agent跟踪记录现在看不到的那部分世界。即Agent应该根据感知历史维持内部状态，从而至少反映出当前状态看不到的信息。需要在程序中加入两种类型的知识：首先需要知道世界是如何独立于Agent而发展的信息                                                    其次，知道Aagent自身的行动如何影响世界的信息。这种模型的Agent被称为基于模型的Agent关于 Agent 目的地的事实才是真正的 Agent 内部状态的一个方面### 2.4.4 基于目标的AgentAgent还需要目标信息来描述想要达到的状况搜索和规划是寻找达成Agent目标的行动序列的人工智能与条件-行动不同，因为他考虑了未来尽管效率低，但是更加灵活### 2.4.5 基于效用的Agent仅靠目标不足以生成高品质的行为。效用：Agent的效用函数是性能度量的内在化，如果内在的效用函数和外在的性能度量是和谐的，那么选择最大效用行动的Agent根据外在的性能度量也是理性的第一，当多个目标互相冲突时，只有其中一些目标可以达到时（例如，速度和安全性)，效用函数可以在它们之间适当的折中。第二，当 Agent 有几个目标，但没有一个有把握达到时，效用函数可以根据目标的重要性对成功的似然率加权理性的基于效用的 Agent 选择使其期望效用最大化的行动### 2.4.6 学习Agent学习 Agent 可以被划分为 4 个概念上的组件：Critic    Learning element  problem generator Performance element，重点体现在学习元件和性能元件。学习元件负责改进提高，而性能元件负责选择外部行动。性能元件是我们前面考虑的整个 Agent: 它接受感知信息并决策。学习元件利用来自评判元件的反馈评价 Agent 做得如何，并确定应该如何修改性能元件以便将来做得更好学习元件的设计很大程度上依赖于性能元件的设计评判元件根据固定的性能标准告诉学习元件 Agent 的运转情况最后一个组件是问题产生器。它负责可以得到新的和有信息的经验的行动提议### 2.4.7 Agent程序的各组件如何工作我们将表示放置在不断增长的复杂度和表达能力的轴线上：原子、要素和结构# 问题求解#第三章 通过搜索进行问题求解问题求解 Agent 使用原子（atomic) 表示：世界的状态被视为一个整体，对问题求解算法而言没有可见的内部结构。使用更先进的要素化或结构化表示的基于目标的 Agent, 通常被称为规划 Agent无信息的搜索算法：除了问题定义本身没有任何其他信息有信息的搜索算法：可以利用给定的值是引导能够有效的找到解## 3.1 问题求解agent三步走：形式化、搜索、执行形式化：帮助 Agent 组织行动序列，以达到最终目标。基于当前的情形和 Agent 的性能度量进行目标形式化（goal formulation) 是问题求解的第一个步骤。问题形式化（problem formulation)是在给定目标下确定需要考虑哪些行动和状态的过程搜索：为达到目标，寻找这样的行动序列的过程被称为搜索：搜索算法的输入是问题，输出是问题的解，以行动序列的形式返回问题的解。执行：执行建议的行动开环系统：十分确定行动的后果是什么，它无视它的感知信息。###3.1.1 良定义的问题以及解一个问题可以用5个组成部分形式化地描述：1.Agent的初始状态2.描述Agent的可能行动，给定一个特殊状态s，Action（s）返回在状态s下可以执行的动作集合。3.对每个行动的描述：即转移模型，这个状态下在这样的操作可以去到哪里。4.目标测试 确定给定的状态是不是目标状态。有时候目标状态是一个显式集合，测试只需简单检查给定的状态是否在目标状态集合中5.路径耗散：路径耗散函数为每条路径赋一个耗散值，即甲醛百年。问题求解Agent选择能反映它自己的性能度量的耗散函数。###3.1.2 问题的形式化一般情况下都要抽象，抽象成数学问题。要对状态描述抽象，要对行动进行抽象。如何精确的定义合适的抽象层次：选择抽象化的状态和行动。如果我们能够把任何抽象解扩展成为更细节的世界中的解，这种抽象就是有效的## 3.2 问题实例分别讨论玩具问题和现实世界问题。### 3.2.1 玩具问题讨论了吸尘器的具体应用，以及八数码问题以及八皇后问题（具体实例）对于八皇后这类的问题：这类问题的形式化主要分为两类：增量形式化（在过程中是从空状态开始的）、完整状态形式化不论这两种哪种，都无需考虑路径消耗，只需要考虑最终状态。增量形式话可以如下考虑：状态：初始状态：行动：转移模型：目标测试：状态：行动：还有玩具问题：由Donald Knuth提出，只用数字4，一个由阶乘、平方根和取整构成的操作序列可以得到任意的正整数。### 3.2.2 现实世界问题寻径问题旅行问题旅行商问题VLSI布线问题机器人导航问题## 3.3 通过搜索求解进行形式化后，对问题进行求解：一个解是一个行动序列，所以搜索算法的工作就是考虑各种可能的行动数列。搜索树：从根节点开始，连线表示行动，节点对应空间中的状态。问题：如何选择将要扩展的状态L:即搜索策略 以避免冗余状态。</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E7%A1%80%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E9%9C%80%E8%A6%81%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%9D%A5%E8%AE%B0%E5%BD%95%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E6%9E%84%E9%80%A0%E8%BF%87%E7%A8%8B%E3%80%82%E5%AF%B9%E6%A0%91%E7%9A%84%E6%AF%8F%E4%B8%AA%E7%BB%93%E7%82%B9-%E6%88%91%E4%BB%AC%E5%AE%9A%E4%B9%89%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8C%85%E5%90%AB%E5%9B%9B%E4%B8%AA%E5%85%83%E7%B4%A0%EF%BC%9A"><span class="nav-text">3.3.1 搜索算法的基础搜索算法需要一个数据结构来记录搜索树的构造过程。对树的每个结点:我们定义的数据结构包含四个元素：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E8%B6%85%E8%B6%8A%E7%BB%8F%E5%85%B8%E6%90%9C%E7%B4%A2"><span class="nav-text">4 超越经典搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%92%8C%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-text">4.1 局部搜索算法和最优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-%E7%88%AC%E5%B1%B1%E6%B3%95"><span class="nav-text">4.1.1 爬山法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E6%90%9C%E7%B4%A2"><span class="nav-text">4.1.2 模拟退火搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-%E5%B1%80%E9%83%A8%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="nav-text">4.1.3 局部束搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-4-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="nav-text">4.1.4 遗传算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2"><span class="nav-text">4.2 连续空间中的局部搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E4%BD%BF%E7%94%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%8A%A8%E4%BD%9C%E7%9A%84%E6%90%9C%E7%B4%A2"><span class="nav-text">4.3 使用不确定动作的搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-%E4%B8%8D%E7%A8%B3%E5%AE%9A%E7%9A%84%E5%90%B8%E5%B0%98%E5%99%A8%E4%B8%96%E7%95%8C"><span class="nav-text">4.3.1 不稳定的吸尘器世界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-2-%E4%B8%8E%E6%88%96%E6%90%9C%E7%B4%A2%E6%A0%91"><span class="nav-text">4.3.2 与或搜索树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-3-%E4%B8%8D%E6%96%AD%E5%B0%9D%E8%AF%95"><span class="nav-text">4.3.3 不断尝试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-%E4%BD%BF%E7%94%A8%E9%83%A8%E5%88%86%E5%8F%AF%E8%A7%82%E5%AF%9F%E4%BF%A1%E6%81%AF%E7%9A%84%E6%90%9C%E7%B4%A2"><span class="nav-text">4.4 使用部分可观察信息的搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-1-%E6%97%A0%E8%A7%82%E5%AF%9F%E4%BF%A1%E6%81%AF%E7%9A%84%E6%90%9C%E7%B4%A2"><span class="nav-text">4.4.1 无观察信息的搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-2-%E6%9C%89%E8%A7%82%E5%AF%9F%E4%BF%A1%E6%81%AF%E7%9A%84%E6%90%9C%E7%B4%A2"><span class="nav-text">4.4.2 有观察信息的搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-3-%E6%B1%82%E8%A7%A3%E9%83%A8%E5%88%86%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">4.4.3 求解部分可观察环境中的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-4-%E9%83%A8%E5%88%86%E5%8F%AF%E8%A7%82%E5%AF%9F%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84Agent"><span class="nav-text">4.4.4 部分可观察环境中的Agent</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-%E8%81%94%E6%9C%BA%E6%90%9C%E7%B4%A2Agent%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%8E%AF%E5%A2%83"><span class="nav-text">4.5 联机搜索Agent和位置环境</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-1-%E8%81%94%E6%9C%BA%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98"><span class="nav-text">4.5.1 联机搜索问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-2-%E8%81%94%E6%9C%BA%E6%90%9C%E7%B4%A2Agent"><span class="nav-text">4.5.2 联机搜索Agent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-3-%E8%81%94%E6%9C%BA%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2"><span class="nav-text">4.5.3 联机局部搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-4-%E8%81%94%E6%9C%BA%E6%90%9C%E7%B4%A2%E4%B8%AD%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="nav-text">4.5.4 联机搜索中的学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2"><span class="nav-text">5 对抗搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E5%8D%9A%E5%BC%88"><span class="nav-text">5. 1 博弈</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2%E3%80%822-%E5%A4%9A%E4%BA%BA%E5%8D%9A%E5%BC%88%E6%97%B6%E7%9A%84%E6%9C%80%E4%BC%98%E5%86%B3%E7%AD%96"><span class="nav-text">5.2。2 多人博弈时的最优决策</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-%E5%89%AA%E6%9E%9D%E9%97%AE%E9%A2%98"><span class="nav-text">5.3 剪枝问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-%E8%A1%8C%E6%A3%8B%E6%8E%92%E5%BA%8F"><span class="nav-text">5.3.1 行棋排序</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-%E4%B8%8D%E5%AE%8C%E7%BE%8E%E7%9A%84%E5%AE%9E%E6%97%B6%E5%86%B3%E7%AD%96"><span class="nav-text">5.4 不完美的实时决策</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-1"><span class="nav-text">5.4.1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-2-%E6%88%AA%E6%96%AD%E6%90%9C%E7%B4%A2"><span class="nav-text">5.4.2 截断搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-3-%E5%90%91%E5%89%8D%E5%89%AA%E6%9E%9D"><span class="nav-text">5.4.3 向前剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-4-%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%9F%A5%E8%A1%A8%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%9F%A5%E8%A1%A8"><span class="nav-text">5.4.4 搜索与查表搜索与查表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-%E9%9A%8F%E6%9C%BA%E5%8D%9A%E5%BC%88"><span class="nav-text">5.5 随机博弈</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1"><span class="nav-text">6.1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-1-%E5%AE%9E%E4%BE%8B%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98"><span class="nav-text">6.1.1 实例：地图着色问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-2-%E4%BD%9C%E4%B8%9A%E8%B0%83%E5%BA%A6%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="nav-text">6.1.2 作业调度问题：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-3-CSP%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96"><span class="nav-text">6.1.3 CSP的形式化</span></a></li></ol></li></ol></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2022</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">杨小鹤</a>
        </div>
        
            <script data-swup-reload-script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.3.0</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>


<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
        ],
        containers: ["#swup"],
    });

    swup.on("pageView", () => {
        Global.refresh();
    });

    // if (document.readyState === "complete") {
    //
    // } else {
    //     document.addEventListener("DOMContentLoaded", () => init());
    // }
</script>





<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>

<script src="/js/layouts/categoryList.js"></script>





    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


</body>
</html>
